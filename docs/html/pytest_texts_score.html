<!DOCTYPE html>

<html lang="en" data-content_root="./">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>pytest_texts_score package &#8212; pytest-texts-score  documentation</title>
    <link rel="stylesheet" type="text/css" href="_static/pygments.css?v=5ecbeea2" />
    <link rel="stylesheet" type="text/css" href="_static/basic.css?v=b08954a9" />
    <link rel="stylesheet" type="text/css" href="_static/alabaster.css?v=27fed22d" />
    <script src="_static/documentation_options.js?v=5929fcd5"></script>
    <script src="_static/doctools.js?v=fd6eb6e6"></script>
    <script src="_static/sphinx_highlight.js?v=6ffebe34"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="prev" title="pytest_texts_score" href="modules.html" />
   
  <link rel="stylesheet" href="_static/custom.css" type="text/css" />
  

  
  

  </head><body>
  

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          

          <div class="body" role="main">
            
  <section id="module-pytest_texts_score">
<span id="pytest-texts-score-package"></span><h1>pytest_texts_score package<a class="headerlink" href="#module-pytest_texts_score" title="Link to this heading">¶</a></h1>
<p>Main entry point for the <code class="docutils literal notranslate"><span class="pre">pytest-texts-score</span></code> public API.</p>
<p>This module exposes the primary functions for text-based scoring and assertions
within pytest. It includes functions for single-run evaluations
(<code class="docutils literal notranslate"><span class="pre">texts_expect_*</span></code>) and multi-run, aggregated evaluations (<code class="docutils literal notranslate"><span class="pre">texts_agg_*</span></code>) for
metrics like F1, precision, and recall.</p>
<p>It also provides aliases like “completeness” for precision and “correctness”
for recall, which can be more intuitive in certain testing contexts.</p>
<dl class="py function">
<dt class="sig sig-object py" id="pytest_texts_score.texts_agg_completeness_average">
<span class="sig-prename descclassname"><span class="pre">pytest_texts_score.</span></span><span class="sig-name descname"><span class="pre">texts_agg_completeness_average</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">expected</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">given</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">target</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_delta</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">0.1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">full_runs</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">5</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">each_question_runs</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">retry_on_error</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">None</span></span></span><a class="reference internal" href="_modules/pytest_texts_score/api_wrappers.html#texts_agg_completeness_average"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#pytest_texts_score.texts_agg_completeness_average" title="Link to this definition">¶</a></dt>
<dd><p>Alias for texts_agg_precision_average.</p>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="pytest_texts_score.texts_agg_completeness_max">
<span class="sig-prename descclassname"><span class="pre">pytest_texts_score.</span></span><span class="sig-name descname"><span class="pre">texts_agg_completeness_max</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">expected</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">given</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">upper_bound</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">full_runs</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">5</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">each_question_runs</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">retry_on_error</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">None</span></span></span><a class="reference internal" href="_modules/pytest_texts_score/api_wrappers.html#texts_agg_completeness_max"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#pytest_texts_score.texts_agg_completeness_max" title="Link to this definition">¶</a></dt>
<dd><p>Alias for texts_agg_precision_max.</p>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="pytest_texts_score.texts_agg_completeness_mean">
<span class="sig-prename descclassname"><span class="pre">pytest_texts_score.</span></span><span class="sig-name descname"><span class="pre">texts_agg_completeness_mean</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">expected</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">given</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">target</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_delta</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">0.1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">full_runs</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">5</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">each_question_runs</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">retry_on_error</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">None</span></span></span><a class="reference internal" href="_modules/pytest_texts_score/api_wrappers.html#texts_agg_completeness_mean"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#pytest_texts_score.texts_agg_completeness_mean" title="Link to this definition">¶</a></dt>
<dd><p>Alias for texts_agg_precision_mean.</p>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="pytest_texts_score.texts_agg_completeness_median">
<span class="sig-prename descclassname"><span class="pre">pytest_texts_score.</span></span><span class="sig-name descname"><span class="pre">texts_agg_completeness_median</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">expected</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">given</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">target</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_delta</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">0.1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">full_runs</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">5</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">each_question_runs</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">retry_on_error</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">None</span></span></span><a class="reference internal" href="_modules/pytest_texts_score/api_wrappers.html#texts_agg_completeness_median"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#pytest_texts_score.texts_agg_completeness_median" title="Link to this definition">¶</a></dt>
<dd><p>Alias for texts_agg_precision_median.</p>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="pytest_texts_score.texts_agg_completeness_min">
<span class="sig-prename descclassname"><span class="pre">pytest_texts_score.</span></span><span class="sig-name descname"><span class="pre">texts_agg_completeness_min</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">expected</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">given</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">lower_bound</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">full_runs</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">5</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">each_question_runs</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">retry_on_error</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">None</span></span></span><a class="reference internal" href="_modules/pytest_texts_score/api_wrappers.html#texts_agg_completeness_min"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#pytest_texts_score.texts_agg_completeness_min" title="Link to this definition">¶</a></dt>
<dd><p>Alias for texts_agg_precision_min.</p>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="pytest_texts_score.texts_agg_correctness_average">
<span class="sig-prename descclassname"><span class="pre">pytest_texts_score.</span></span><span class="sig-name descname"><span class="pre">texts_agg_correctness_average</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">expected</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">given</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">target</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_delta</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">0.1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">full_runs</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">5</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">each_question_runs</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">retry_on_error</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">None</span></span></span><a class="reference internal" href="_modules/pytest_texts_score/api_wrappers.html#texts_agg_correctness_average"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#pytest_texts_score.texts_agg_correctness_average" title="Link to this definition">¶</a></dt>
<dd><p>Alias for texts_agg_recall_average.</p>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="pytest_texts_score.texts_agg_correctness_max">
<span class="sig-prename descclassname"><span class="pre">pytest_texts_score.</span></span><span class="sig-name descname"><span class="pre">texts_agg_correctness_max</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">expected</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">given</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">upper_bound</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">full_runs</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">5</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">each_question_runs</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">retry_on_error</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">None</span></span></span><a class="reference internal" href="_modules/pytest_texts_score/api_wrappers.html#texts_agg_correctness_max"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#pytest_texts_score.texts_agg_correctness_max" title="Link to this definition">¶</a></dt>
<dd><p>Alias for texts_agg_recall_max.</p>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="pytest_texts_score.texts_agg_correctness_mean">
<span class="sig-prename descclassname"><span class="pre">pytest_texts_score.</span></span><span class="sig-name descname"><span class="pre">texts_agg_correctness_mean</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">expected</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">given</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">target</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_delta</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">0.1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">full_runs</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">5</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">each_question_runs</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">retry_on_error</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">None</span></span></span><a class="reference internal" href="_modules/pytest_texts_score/api_wrappers.html#texts_agg_correctness_mean"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#pytest_texts_score.texts_agg_correctness_mean" title="Link to this definition">¶</a></dt>
<dd><p>Alias for texts_agg_recall_mean.</p>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="pytest_texts_score.texts_agg_correctness_median">
<span class="sig-prename descclassname"><span class="pre">pytest_texts_score.</span></span><span class="sig-name descname"><span class="pre">texts_agg_correctness_median</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">expected</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">given</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">target</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_delta</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">0.1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">full_runs</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">5</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">each_question_runs</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">retry_on_error</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">None</span></span></span><a class="reference internal" href="_modules/pytest_texts_score/api_wrappers.html#texts_agg_correctness_median"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#pytest_texts_score.texts_agg_correctness_median" title="Link to this definition">¶</a></dt>
<dd><p>Alias for texts_agg_recall_median.</p>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="pytest_texts_score.texts_agg_correctness_min">
<span class="sig-prename descclassname"><span class="pre">pytest_texts_score.</span></span><span class="sig-name descname"><span class="pre">texts_agg_correctness_min</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">expected</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">given</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">lower_bound</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">full_runs</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">5</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">each_question_runs</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">retry_on_error</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">None</span></span></span><a class="reference internal" href="_modules/pytest_texts_score/api_wrappers.html#texts_agg_correctness_min"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#pytest_texts_score.texts_agg_correctness_min" title="Link to this definition">¶</a></dt>
<dd><p>Alias for texts_agg_recall_min.</p>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="pytest_texts_score.texts_agg_f1_average">
<span class="sig-prename descclassname"><span class="pre">pytest_texts_score.</span></span><span class="sig-name descname"><span class="pre">texts_agg_f1_average</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">expected</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">given</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">target</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_delta</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">0.1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">full_runs</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">5</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">each_question_runs</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">retry_on_error</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">None</span></span></span><a class="reference internal" href="_modules/pytest_texts_score/api_wrappers.html#texts_agg_f1_average"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#pytest_texts_score.texts_agg_f1_average" title="Link to this definition">¶</a></dt>
<dd><p>Alias for texts_agg_f1_mean.</p>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="pytest_texts_score.texts_agg_f1_max">
<span class="sig-prename descclassname"><span class="pre">pytest_texts_score.</span></span><span class="sig-name descname"><span class="pre">texts_agg_f1_max</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">expected</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">given</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">upper_bound</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">full_runs</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">5</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">each_question_runs</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">retry_on_error</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">None</span></span></span><a class="reference internal" href="_modules/pytest_texts_score/api.html#texts_agg_f1_max"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#pytest_texts_score.texts_agg_f1_max" title="Link to this definition">¶</a></dt>
<dd><p>Assert that the maximum aggregated F1 score is below an upper bound.</p>
<p>Performs multiple evaluation runs, calculates the maximum F1 score across
all runs, and asserts that this maximum score is less than or equal to
<code class="docutils literal notranslate"><span class="pre">upper_bound</span></code>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>expected</strong> (<em>str</em>) – The reference text.</p></li>
<li><p><strong>given</strong> (<em>str</em>) – The text to evaluate.</p></li>
<li><p><strong>upper_bound</strong> (<em>float</em>) – The maximum acceptable score for the aggregated maximum.</p></li>
<li><p><strong>full_runs</strong> (<em>int</em>) – Number of times to generate new questions. Defaults to 5.</p></li>
<li><p><strong>each_question_runs</strong> (<em>int</em>) – Number of times to evaluate answers per question set. Defaults to 1.</p></li>
<li><p><strong>retry_on_error</strong> (<em>bool</em>) – If <code class="docutils literal notranslate"><span class="pre">True</span></code>, retries LLM calls on failure.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="pytest_texts_score.texts_agg_f1_mean">
<span class="sig-prename descclassname"><span class="pre">pytest_texts_score.</span></span><span class="sig-name descname"><span class="pre">texts_agg_f1_mean</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">expected</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">given</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">target</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_delta</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">0.1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">full_runs</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">5</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">each_question_runs</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">retry_on_error</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">None</span></span></span><a class="reference internal" href="_modules/pytest_texts_score/api.html#texts_agg_f1_mean"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#pytest_texts_score.texts_agg_f1_mean" title="Link to this definition">¶</a></dt>
<dd><p>Assert that the mean aggregated F1 score is close to a target value.</p>
<p>Performs multiple evaluation runs, calculates the mean (average) F1 score,
and asserts that it falls within the range defined by <code class="docutils literal notranslate"><span class="pre">target</span> <span class="pre">±</span> <span class="pre">max_delta</span></code>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>expected</strong> (<em>str</em>) – The reference text.</p></li>
<li><p><strong>given</strong> (<em>str</em>) – The text to evaluate.</p></li>
<li><p><strong>target</strong> (<em>float</em>) – The expected mean score.</p></li>
<li><p><strong>max_delta</strong> (<em>float</em>) – The allowed deviation from the target. Defaults to 0.1.</p></li>
<li><p><strong>full_runs</strong> (<em>int</em>) – Number of times to generate new questions. Defaults to 5.</p></li>
<li><p><strong>each_question_runs</strong> (<em>int</em>) – Number of times to evaluate answers per question set. Defaults to 1.</p></li>
<li><p><strong>retry_on_error</strong> (<em>bool</em>) – If <code class="docutils literal notranslate"><span class="pre">True</span></code>, retries LLM calls on failure.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="pytest_texts_score.texts_agg_f1_median">
<span class="sig-prename descclassname"><span class="pre">pytest_texts_score.</span></span><span class="sig-name descname"><span class="pre">texts_agg_f1_median</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">expected</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">given</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">target</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_delta</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">0.1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">full_runs</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">5</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">each_question_runs</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">retry_on_error</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">None</span></span></span><a class="reference internal" href="_modules/pytest_texts_score/api.html#texts_agg_f1_median"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#pytest_texts_score.texts_agg_f1_median" title="Link to this definition">¶</a></dt>
<dd><p>Assert that the median aggregated F1 score is close to a target value.</p>
<p>Performs multiple evaluation runs, calculates the median F1 score, and
asserts that it falls within the range defined by <code class="docutils literal notranslate"><span class="pre">target</span> <span class="pre">±</span> <span class="pre">max_delta</span></code>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>expected</strong> (<em>str</em>) – The reference text.</p></li>
<li><p><strong>given</strong> (<em>str</em>) – The text to evaluate.</p></li>
<li><p><strong>target</strong> (<em>float</em>) – The expected median score.</p></li>
<li><p><strong>max_delta</strong> (<em>float</em>) – The allowed deviation from the target. Defaults to 0.1.</p></li>
<li><p><strong>full_runs</strong> (<em>int</em>) – Number of times to generate new questions. Defaults to 5.</p></li>
<li><p><strong>each_question_runs</strong> (<em>int</em>) – Number of times to evaluate answers per question set. Defaults to 1.</p></li>
<li><p><strong>retry_on_error</strong> (<em>bool</em>) – If <code class="docutils literal notranslate"><span class="pre">True</span></code>, retries LLM calls on failure.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="pytest_texts_score.texts_agg_f1_min">
<span class="sig-prename descclassname"><span class="pre">pytest_texts_score.</span></span><span class="sig-name descname"><span class="pre">texts_agg_f1_min</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">expected</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">given</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">lower_bound</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">full_runs</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">5</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">each_question_runs</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">retry_on_error</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">None</span></span></span><a class="reference internal" href="_modules/pytest_texts_score/api.html#texts_agg_f1_min"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#pytest_texts_score.texts_agg_f1_min" title="Link to this definition">¶</a></dt>
<dd><p>Assert that the minimum aggregated F1 score is above a lower bound.</p>
<p>Performs multiple evaluation runs, calculates the minimum F1 score across
all runs, and asserts that this minimum score is greater than or equal to
<code class="docutils literal notranslate"><span class="pre">lower_bound</span></code>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>expected</strong> (<em>str</em>) – The reference text.</p></li>
<li><p><strong>given</strong> (<em>str</em>) – The text to evaluate.</p></li>
<li><p><strong>lower_bound</strong> (<em>float</em>) – The minimum acceptable score for the aggregated minimum.</p></li>
<li><p><strong>full_runs</strong> (<em>int</em>) – Number of times to generate new questions. Defaults to 5.</p></li>
<li><p><strong>each_question_runs</strong> (<em>int</em>) – Number of times to evaluate answers per question set. Defaults to 1.</p></li>
<li><p><strong>retry_on_error</strong> (<em>bool</em>) – If <code class="docutils literal notranslate"><span class="pre">True</span></code>, retries LLM calls on failure.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="pytest_texts_score.texts_agg_precision_average">
<span class="sig-prename descclassname"><span class="pre">pytest_texts_score.</span></span><span class="sig-name descname"><span class="pre">texts_agg_precision_average</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">expected</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">given</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">target</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_delta</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">0.1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">full_runs</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">5</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">each_question_runs</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">retry_on_error</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">None</span></span></span><a class="reference internal" href="_modules/pytest_texts_score/api_wrappers.html#texts_agg_precision_average"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#pytest_texts_score.texts_agg_precision_average" title="Link to this definition">¶</a></dt>
<dd><p>Alias for texts_agg_precision_mean.</p>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="pytest_texts_score.texts_agg_precision_max">
<span class="sig-prename descclassname"><span class="pre">pytest_texts_score.</span></span><span class="sig-name descname"><span class="pre">texts_agg_precision_max</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">expected</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">given</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">upper_bound</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">full_runs</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">5</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">each_question_runs</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">retry_on_error</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">None</span></span></span><a class="reference internal" href="_modules/pytest_texts_score/api.html#texts_agg_precision_max"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#pytest_texts_score.texts_agg_precision_max" title="Link to this definition">¶</a></dt>
<dd><p>Assert that the maximum aggregated precision is below an upper bound.</p>
<p>Performs multiple evaluation runs, calculates the maximum precision score
across all runs, and asserts that this maximum score is less than or equal
to <code class="docutils literal notranslate"><span class="pre">upper_bound</span></code>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>expected</strong> (<em>str</em>) – The reference text.</p></li>
<li><p><strong>given</strong> (<em>str</em>) – The text to evaluate.</p></li>
<li><p><strong>upper_bound</strong> (<em>float</em>) – The maximum acceptable score for the aggregated maximum.</p></li>
<li><p><strong>full_runs</strong> (<em>int</em>) – Number of times to generate new questions. Defaults to 5.</p></li>
<li><p><strong>each_question_runs</strong> (<em>int</em>) – Number of times to evaluate answers per question set. Defaults to 1.</p></li>
<li><p><strong>retry_on_error</strong> (<em>bool</em>) – If <code class="docutils literal notranslate"><span class="pre">True</span></code>, retries LLM calls on failure.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="pytest_texts_score.texts_agg_precision_mean">
<span class="sig-prename descclassname"><span class="pre">pytest_texts_score.</span></span><span class="sig-name descname"><span class="pre">texts_agg_precision_mean</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">expected</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">given</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">target</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_delta</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">0.1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">full_runs</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">5</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">each_question_runs</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">retry_on_error</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">None</span></span></span><a class="reference internal" href="_modules/pytest_texts_score/api.html#texts_agg_precision_mean"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#pytest_texts_score.texts_agg_precision_mean" title="Link to this definition">¶</a></dt>
<dd><p>Assert that the mean aggregated precision is close to a target value.</p>
<p>Performs multiple evaluation runs, calculates the mean (average) precision
score, and asserts that it falls within the range <code class="docutils literal notranslate"><span class="pre">target</span> <span class="pre">±</span> <span class="pre">max_delta</span></code>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>expected</strong> (<em>str</em>) – The reference text.</p></li>
<li><p><strong>given</strong> (<em>str</em>) – The text to evaluate.</p></li>
<li><p><strong>target</strong> (<em>float</em>) – The expected mean score.</p></li>
<li><p><strong>max_delta</strong> (<em>float</em>) – The allowed deviation from the target. Defaults to 0.1.</p></li>
<li><p><strong>full_runs</strong> (<em>int</em>) – Number of times to generate new questions. Defaults to 5.</p></li>
<li><p><strong>each_question_runs</strong> (<em>int</em>) – Number of times to evaluate answers per question set. Defaults to 1.</p></li>
<li><p><strong>retry_on_error</strong> (<em>bool</em>) – If <code class="docutils literal notranslate"><span class="pre">True</span></code>, retries LLM calls on failure.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="pytest_texts_score.texts_agg_precision_median">
<span class="sig-prename descclassname"><span class="pre">pytest_texts_score.</span></span><span class="sig-name descname"><span class="pre">texts_agg_precision_median</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">expected</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">given</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">target</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_delta</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">0.1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">full_runs</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">5</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">each_question_runs</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">retry_on_error</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">None</span></span></span><a class="reference internal" href="_modules/pytest_texts_score/api.html#texts_agg_precision_median"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#pytest_texts_score.texts_agg_precision_median" title="Link to this definition">¶</a></dt>
<dd><p>Assert that the median aggregated precision is close to a target value.</p>
<p>Performs multiple evaluation runs, calculates the median precision score,
and asserts that it falls within the range defined by <code class="docutils literal notranslate"><span class="pre">target</span> <span class="pre">±</span> <span class="pre">max_delta</span></code>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>expected</strong> (<em>str</em>) – The reference text.</p></li>
<li><p><strong>given</strong> (<em>str</em>) – The text to evaluate.</p></li>
<li><p><strong>target</strong> (<em>float</em>) – The expected median score.</p></li>
<li><p><strong>max_delta</strong> (<em>float</em>) – The allowed deviation from the target. Defaults to 0.1.</p></li>
<li><p><strong>full_runs</strong> (<em>int</em>) – Number of times to generate new questions. Defaults to 5.</p></li>
<li><p><strong>each_question_runs</strong> (<em>int</em>) – Number of times to evaluate answers per question set. Defaults to 1.</p></li>
<li><p><strong>retry_on_error</strong> (<em>bool</em>) – If <code class="docutils literal notranslate"><span class="pre">True</span></code>, retries LLM calls on failure.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="pytest_texts_score.texts_agg_precision_min">
<span class="sig-prename descclassname"><span class="pre">pytest_texts_score.</span></span><span class="sig-name descname"><span class="pre">texts_agg_precision_min</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">expected</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">given</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">lower_bound</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">full_runs</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">5</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">each_question_runs</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">retry_on_error</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">None</span></span></span><a class="reference internal" href="_modules/pytest_texts_score/api.html#texts_agg_precision_min"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#pytest_texts_score.texts_agg_precision_min" title="Link to this definition">¶</a></dt>
<dd><p>Assert that the minimum aggregated precision is above a lower bound.</p>
<p>Performs multiple evaluation runs, calculates the minimum precision score
across all runs, and asserts that this minimum score is greater than or
equal to <code class="docutils literal notranslate"><span class="pre">lower_bound</span></code>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>expected</strong> (<em>str</em>) – The reference text.</p></li>
<li><p><strong>given</strong> (<em>str</em>) – The text to evaluate.</p></li>
<li><p><strong>lower_bound</strong> (<em>float</em>) – The minimum acceptable score for the aggregated minimum.</p></li>
<li><p><strong>full_runs</strong> (<em>int</em>) – Number of times to generate new questions. Defaults to 5.</p></li>
<li><p><strong>each_question_runs</strong> (<em>int</em>) – Number of times to evaluate answers per question set. Defaults to 1.</p></li>
<li><p><strong>retry_on_error</strong> (<em>bool</em>) – If <code class="docutils literal notranslate"><span class="pre">True</span></code>, retries LLM calls on failure.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="pytest_texts_score.texts_agg_recall_average">
<span class="sig-prename descclassname"><span class="pre">pytest_texts_score.</span></span><span class="sig-name descname"><span class="pre">texts_agg_recall_average</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">expected</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">given</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">target</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_delta</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">0.1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">full_runs</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">5</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">each_question_runs</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">retry_on_error</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">None</span></span></span><a class="reference internal" href="_modules/pytest_texts_score/api_wrappers.html#texts_agg_recall_average"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#pytest_texts_score.texts_agg_recall_average" title="Link to this definition">¶</a></dt>
<dd><p>Alias for texts_agg_recall_mean.</p>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="pytest_texts_score.texts_agg_recall_max">
<span class="sig-prename descclassname"><span class="pre">pytest_texts_score.</span></span><span class="sig-name descname"><span class="pre">texts_agg_recall_max</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">expected</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">given</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">upper_bound</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">full_runs</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">5</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">each_question_runs</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">retry_on_error</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">None</span></span></span><a class="reference internal" href="_modules/pytest_texts_score/api.html#texts_agg_recall_max"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#pytest_texts_score.texts_agg_recall_max" title="Link to this definition">¶</a></dt>
<dd><p>Assert that the maximum aggregated recall is below an upper bound.</p>
<p>Performs multiple evaluation runs, calculates the maximum recall score
across all runs, and asserts that this maximum score is less than or equal
to <code class="docutils literal notranslate"><span class="pre">upper_bound</span></code>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>expected</strong> (<em>str</em>) – The reference text.</p></li>
<li><p><strong>given</strong> (<em>str</em>) – The text to evaluate.</p></li>
<li><p><strong>upper_bound</strong> (<em>float</em>) – The maximum acceptable score for the aggregated maximum.</p></li>
<li><p><strong>full_runs</strong> (<em>int</em>) – Number of times to generate new questions. Defaults to 5.</p></li>
<li><p><strong>each_question_runs</strong> (<em>int</em>) – Number of times to evaluate answers per question set. Defaults to 1.</p></li>
<li><p><strong>retry_on_error</strong> (<em>bool</em>) – If <code class="docutils literal notranslate"><span class="pre">True</span></code>, retries LLM calls on failure.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="pytest_texts_score.texts_agg_recall_mean">
<span class="sig-prename descclassname"><span class="pre">pytest_texts_score.</span></span><span class="sig-name descname"><span class="pre">texts_agg_recall_mean</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">expected</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">given</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">target</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_delta</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">0.1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">full_runs</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">5</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">each_question_runs</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">retry_on_error</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">None</span></span></span><a class="reference internal" href="_modules/pytest_texts_score/api.html#texts_agg_recall_mean"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#pytest_texts_score.texts_agg_recall_mean" title="Link to this definition">¶</a></dt>
<dd><p>Assert that the mean aggregated recall is close to a target value.</p>
<p>Performs multiple evaluation runs, calculates the mean (average) recall
score, and asserts that it falls within the range <code class="docutils literal notranslate"><span class="pre">target</span> <span class="pre">±</span> <span class="pre">max_delta</span></code>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>expected</strong> (<em>str</em>) – The reference text.</p></li>
<li><p><strong>given</strong> (<em>str</em>) – The text to evaluate.</p></li>
<li><p><strong>target</strong> (<em>float</em>) – The expected mean score.</p></li>
<li><p><strong>max_delta</strong> (<em>float</em>) – The allowed deviation from the target. Defaults to 0.1.</p></li>
<li><p><strong>full_runs</strong> (<em>int</em>) – Number of times to generate new questions. Defaults to 5.</p></li>
<li><p><strong>each_question_runs</strong> (<em>int</em>) – Number of times to evaluate answers per question set. Defaults to 1.</p></li>
<li><p><strong>retry_on_error</strong> (<em>bool</em>) – If <code class="docutils literal notranslate"><span class="pre">True</span></code>, retries LLM calls on failure.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="pytest_texts_score.texts_agg_recall_median">
<span class="sig-prename descclassname"><span class="pre">pytest_texts_score.</span></span><span class="sig-name descname"><span class="pre">texts_agg_recall_median</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">expected</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">given</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">target</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_delta</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">0.1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">full_runs</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">5</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">each_question_runs</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">retry_on_error</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">None</span></span></span><a class="reference internal" href="_modules/pytest_texts_score/api.html#texts_agg_recall_median"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#pytest_texts_score.texts_agg_recall_median" title="Link to this definition">¶</a></dt>
<dd><p>Assert that the median aggregated recall is close to a target value.</p>
<p>Performs multiple evaluation runs, calculates the median recall score, and
asserts that it falls within the range defined by <code class="docutils literal notranslate"><span class="pre">target</span> <span class="pre">±</span> <span class="pre">max_delta</span></code>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>expected</strong> (<em>str</em>) – The reference text.</p></li>
<li><p><strong>given</strong> (<em>str</em>) – The text to evaluate.</p></li>
<li><p><strong>target</strong> (<em>float</em>) – The expected median score.</p></li>
<li><p><strong>max_delta</strong> (<em>float</em>) – The allowed deviation from the target. Defaults to 0.1.</p></li>
<li><p><strong>full_runs</strong> (<em>int</em>) – Number of times to generate new questions. Defaults to 5.</p></li>
<li><p><strong>each_question_runs</strong> (<em>int</em>) – Number of times to evaluate answers per question set. Defaults to 1.</p></li>
<li><p><strong>retry_on_error</strong> (<em>bool</em>) – If <code class="docutils literal notranslate"><span class="pre">True</span></code>, retries LLM calls on failure.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="pytest_texts_score.texts_agg_recall_min">
<span class="sig-prename descclassname"><span class="pre">pytest_texts_score.</span></span><span class="sig-name descname"><span class="pre">texts_agg_recall_min</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">expected</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">given</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">lower_bound</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">full_runs</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">5</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">each_question_runs</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">retry_on_error</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">None</span></span></span><a class="reference internal" href="_modules/pytest_texts_score/api.html#texts_agg_recall_min"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#pytest_texts_score.texts_agg_recall_min" title="Link to this definition">¶</a></dt>
<dd><p>Assert that the minimum aggregated recall is above a lower bound.</p>
<p>Performs multiple evaluation runs, calculates the minimum recall score
across all runs, and asserts that this minimum score is greater than or
equal to <code class="docutils literal notranslate"><span class="pre">lower_bound</span></code>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>expected</strong> (<em>str</em>) – The reference text.</p></li>
<li><p><strong>given</strong> (<em>str</em>) – The text to evaluate.</p></li>
<li><p><strong>lower_bound</strong> (<em>float</em>) – The minimum acceptable score for the aggregated minimum.</p></li>
<li><p><strong>full_runs</strong> (<em>int</em>) – Number of times to generate new questions. Defaults to 5.</p></li>
<li><p><strong>each_question_runs</strong> (<em>int</em>) – Number of times to evaluate answers per question set. Defaults to 1.</p></li>
<li><p><strong>retry_on_error</strong> (<em>bool</em>) – If <code class="docutils literal notranslate"><span class="pre">True</span></code>, retries LLM calls on failure.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="pytest_texts_score.texts_expect_completeness_equal">
<span class="sig-prename descclassname"><span class="pre">pytest_texts_score.</span></span><span class="sig-name descname"><span class="pre">texts_expect_completeness_equal</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">expected</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">given</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">target</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">1.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_delta</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">0.2</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">skip_warnings</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">retry_on_error</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">None</span></span></span><a class="reference internal" href="_modules/pytest_texts_score/api_wrappers.html#texts_expect_completeness_equal"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#pytest_texts_score.texts_expect_completeness_equal" title="Link to this definition">¶</a></dt>
<dd><p>Alias for texts_expect_precision_equal.</p>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="pytest_texts_score.texts_expect_completeness_range">
<span class="sig-prename descclassname"><span class="pre">pytest_texts_score.</span></span><span class="sig-name descname"><span class="pre">texts_expect_completeness_range</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">expected</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">given</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">min_score</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_score</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">skip_warnings</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">retry_on_error</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">None</span></span></span><a class="reference internal" href="_modules/pytest_texts_score/api_wrappers.html#texts_expect_completeness_range"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#pytest_texts_score.texts_expect_completeness_range" title="Link to this definition">¶</a></dt>
<dd><p>Alias for texts_expect_precision_range.</p>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="pytest_texts_score.texts_expect_correctness_equal">
<span class="sig-prename descclassname"><span class="pre">pytest_texts_score.</span></span><span class="sig-name descname"><span class="pre">texts_expect_correctness_equal</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">expected</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">given</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">target</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">1.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_delta</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">0.2</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">skip_warnings</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">retry_on_error</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">None</span></span></span><a class="reference internal" href="_modules/pytest_texts_score/api_wrappers.html#texts_expect_correctness_equal"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#pytest_texts_score.texts_expect_correctness_equal" title="Link to this definition">¶</a></dt>
<dd><p>Alias for texts_expect_recall_equal.</p>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="pytest_texts_score.texts_expect_correctness_range">
<span class="sig-prename descclassname"><span class="pre">pytest_texts_score.</span></span><span class="sig-name descname"><span class="pre">texts_expect_correctness_range</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">expected</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">given</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">min_score</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_score</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">skip_warnings</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">retry_on_error</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">None</span></span></span><a class="reference internal" href="_modules/pytest_texts_score/api_wrappers.html#texts_expect_correctness_range"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#pytest_texts_score.texts_expect_correctness_range" title="Link to this definition">¶</a></dt>
<dd><p>Alias for texts_expect_recall_range.</p>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="pytest_texts_score.texts_expect_f1_equal">
<span class="sig-prename descclassname"><span class="pre">pytest_texts_score.</span></span><span class="sig-name descname"><span class="pre">texts_expect_f1_equal</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">expected</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">given</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">target</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">1.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_delta</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">0.2</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">skip_warnings</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">retry_on_error</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">None</span></span></span><a class="reference internal" href="_modules/pytest_texts_score/api.html#texts_expect_f1_equal"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#pytest_texts_score.texts_expect_f1_equal" title="Link to this definition">¶</a></dt>
<dd><p>Assert that the F1 score is close to a target value.</p>
<p>This is a convenience wrapper around <a class="reference internal" href="#pytest_texts_score.texts_expect_f1_range" title="pytest_texts_score.texts_expect_f1_range"><code class="xref py py-func docutils literal notranslate"><span class="pre">texts_expect_f1_range()</span></code></a>.
It performs a single F1 score evaluation and asserts that the result
is within <code class="docutils literal notranslate"><span class="pre">target</span> <span class="pre">±</span> <span class="pre">max_delta</span></code>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>expected</strong> (<em>str</em>) – The reference text.</p></li>
<li><p><strong>given</strong> (<em>str</em>) – The text to evaluate.</p></li>
<li><p><strong>target</strong> (<em>float</em>) – The expected F1 score. Defaults to 1.0.</p></li>
<li><p><strong>max_delta</strong> (<em>float</em>) – The allowed deviation from the target. Defaults to 0.2.</p></li>
<li><p><strong>skip_warnings</strong> (<em>bool</em>) – If <code class="docutils literal notranslate"><span class="pre">True</span></code>, suppresses input validation warnings.</p></li>
<li><p><strong>retry_on_error</strong> (<em>bool</em>) – If <code class="docutils literal notranslate"><span class="pre">True</span></code>, retries LLM calls on failure.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="pytest_texts_score.texts_expect_f1_range">
<span class="sig-prename descclassname"><span class="pre">pytest_texts_score.</span></span><span class="sig-name descname"><span class="pre">texts_expect_f1_range</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">expected</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">given</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">min_score</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_score</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">skip_warnings</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">retry_on_error</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">None</span></span></span><a class="reference internal" href="_modules/pytest_texts_score/api.html#texts_expect_f1_range"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#pytest_texts_score.texts_expect_f1_range" title="Link to this definition">¶</a></dt>
<dd><p>Assert that the F1 score falls within a specified range.</p>
<p>This function performs a single evaluation of the F1 score between the
<code class="docutils literal notranslate"><span class="pre">expected</span></code> and <code class="docutils literal notranslate"><span class="pre">given</span></code> texts. It then asserts that the resulting score
is between <code class="docutils literal notranslate"><span class="pre">min_score</span></code> and <code class="docutils literal notranslate"><span class="pre">max_score</span></code>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>expected</strong> (<em>str</em>) – The reference text.</p></li>
<li><p><strong>given</strong> (<em>str</em>) – The text to evaluate.</p></li>
<li><p><strong>min_score</strong> (<em>float</em>) – The minimum acceptable F1 score.</p></li>
<li><p><strong>max_score</strong> (<em>float</em>) – The maximum acceptable F1 score.</p></li>
<li><p><strong>skip_warnings</strong> (<em>bool</em>) – If <code class="docutils literal notranslate"><span class="pre">True</span></code>, suppresses input validation warnings.</p></li>
<li><p><strong>retry_on_error</strong> (<em>bool</em>) – If <code class="docutils literal notranslate"><span class="pre">True</span></code>, retries LLM calls on failure.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="pytest_texts_score.texts_expect_precision_equal">
<span class="sig-prename descclassname"><span class="pre">pytest_texts_score.</span></span><span class="sig-name descname"><span class="pre">texts_expect_precision_equal</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">expected</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">given</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">target</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">1.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_delta</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">0.2</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">skip_warnings</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">retry_on_error</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">None</span></span></span><a class="reference internal" href="_modules/pytest_texts_score/api.html#texts_expect_precision_equal"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#pytest_texts_score.texts_expect_precision_equal" title="Link to this definition">¶</a></dt>
<dd><p>Assert that the precision score is close to a target value.</p>
<p>This is a convenience wrapper around <a class="reference internal" href="#pytest_texts_score.texts_expect_precision_range" title="pytest_texts_score.texts_expect_precision_range"><code class="xref py py-func docutils literal notranslate"><span class="pre">texts_expect_precision_range()</span></code></a>.
It performs a single precision score evaluation and asserts that the result
is within <code class="docutils literal notranslate"><span class="pre">target</span> <span class="pre">±</span> <span class="pre">max_delta</span></code>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>expected</strong> (<em>str</em>) – The reference text.</p></li>
<li><p><strong>given</strong> (<em>str</em>) – The text to evaluate.</p></li>
<li><p><strong>target</strong> (<em>float</em>) – The expected precision score. Defaults to 1.0.</p></li>
<li><p><strong>max_delta</strong> (<em>float</em>) – The allowed deviation from the target. Defaults to 0.2.</p></li>
<li><p><strong>skip_warnings</strong> (<em>bool</em>) – If <code class="docutils literal notranslate"><span class="pre">True</span></code>, suppresses input validation warnings.</p></li>
<li><p><strong>retry_on_error</strong> (<em>bool</em>) – If <code class="docutils literal notranslate"><span class="pre">True</span></code>, retries LLM calls on failure.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="pytest_texts_score.texts_expect_precision_range">
<span class="sig-prename descclassname"><span class="pre">pytest_texts_score.</span></span><span class="sig-name descname"><span class="pre">texts_expect_precision_range</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">expected</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">given</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">min_score</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_score</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">skip_warnings</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">retry_on_error</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">None</span></span></span><a class="reference internal" href="_modules/pytest_texts_score/api.html#texts_expect_precision_range"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#pytest_texts_score.texts_expect_precision_range" title="Link to this definition">¶</a></dt>
<dd><p>Assert that the precision score falls within a specified range.</p>
<p>This function performs a single evaluation of the precision score between the
<code class="docutils literal notranslate"><span class="pre">expected</span></code> and <code class="docutils literal notranslate"><span class="pre">given</span></code> texts. It then asserts that the resulting score
is between <code class="docutils literal notranslate"><span class="pre">min_score</span></code> and <code class="docutils literal notranslate"><span class="pre">max_score</span></code>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>expected</strong> (<em>str</em>) – The reference text.</p></li>
<li><p><strong>given</strong> (<em>str</em>) – The text to evaluate.</p></li>
<li><p><strong>min_score</strong> (<em>float</em>) – The minimum acceptable precision score.</p></li>
<li><p><strong>max_score</strong> (<em>float</em>) – The maximum acceptable precision score.</p></li>
<li><p><strong>skip_warnings</strong> (<em>bool</em>) – If <code class="docutils literal notranslate"><span class="pre">True</span></code>, suppresses input validation warnings.</p></li>
<li><p><strong>retry_on_error</strong> (<em>bool</em>) – If <code class="docutils literal notranslate"><span class="pre">True</span></code>, retries LLM calls on failure.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="pytest_texts_score.texts_expect_recall_equal">
<span class="sig-prename descclassname"><span class="pre">pytest_texts_score.</span></span><span class="sig-name descname"><span class="pre">texts_expect_recall_equal</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">expected</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">given</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">target</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">1.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_delta</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">0.2</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">skip_warnings</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">retry_on_error</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">None</span></span></span><a class="reference internal" href="_modules/pytest_texts_score/api.html#texts_expect_recall_equal"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#pytest_texts_score.texts_expect_recall_equal" title="Link to this definition">¶</a></dt>
<dd><p>Assert that the recall score is close to a target value.</p>
<p>This is a convenience wrapper around <a class="reference internal" href="#pytest_texts_score.texts_expect_recall_range" title="pytest_texts_score.texts_expect_recall_range"><code class="xref py py-func docutils literal notranslate"><span class="pre">texts_expect_recall_range()</span></code></a>.
It performs a single recall score evaluation and asserts that the result
is within <code class="docutils literal notranslate"><span class="pre">target</span> <span class="pre">±</span> <span class="pre">max_delta</span></code>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>expected</strong> (<em>str</em>) – The reference text.</p></li>
<li><p><strong>given</strong> (<em>str</em>) – The text to evaluate.</p></li>
<li><p><strong>target</strong> (<em>float</em>) – The expected recall score. Defaults to 1.0.</p></li>
<li><p><strong>max_delta</strong> (<em>float</em>) – The allowed deviation from the target. Defaults to 0.2.</p></li>
<li><p><strong>skip_warnings</strong> (<em>bool</em>) – If <code class="docutils literal notranslate"><span class="pre">True</span></code>, suppresses input validation warnings.</p></li>
<li><p><strong>retry_on_error</strong> (<em>bool</em>) – If <code class="docutils literal notranslate"><span class="pre">True</span></code>, retries LLM calls on failure.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="pytest_texts_score.texts_expect_recall_range">
<span class="sig-prename descclassname"><span class="pre">pytest_texts_score.</span></span><span class="sig-name descname"><span class="pre">texts_expect_recall_range</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">expected</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">given</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">min_score</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_score</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">skip_warnings</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">retry_on_error</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">None</span></span></span><a class="reference internal" href="_modules/pytest_texts_score/api.html#texts_expect_recall_range"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#pytest_texts_score.texts_expect_recall_range" title="Link to this definition">¶</a></dt>
<dd><p>Assert that the recall score falls within a specified range.</p>
<p>This function performs a single evaluation of the recall score between the
<code class="docutils literal notranslate"><span class="pre">expected</span></code> and <code class="docutils literal notranslate"><span class="pre">given</span></code> texts. It then asserts that the resulting score
is between <code class="docutils literal notranslate"><span class="pre">min_score</span></code> and <code class="docutils literal notranslate"><span class="pre">max_score</span></code>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>expected</strong> (<em>str</em>) – The reference text.</p></li>
<li><p><strong>given</strong> (<em>str</em>) – The text to evaluate.</p></li>
<li><p><strong>min_score</strong> (<em>float</em>) – The minimum acceptable recall score.</p></li>
<li><p><strong>max_score</strong> (<em>float</em>) – The maximum acceptable recall score.</p></li>
<li><p><strong>skip_warnings</strong> (<em>bool</em>) – If <code class="docutils literal notranslate"><span class="pre">True</span></code>, suppresses input validation warnings.</p></li>
<li><p><strong>retry_on_error</strong> (<em>bool</em>) – If <code class="docutils literal notranslate"><span class="pre">True</span></code>, retries LLM calls on failure.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<section id="submodules">
<h2>Submodules<a class="headerlink" href="#submodules" title="Link to this heading">¶</a></h2>
</section>
<section id="module-pytest_texts_score.api">
<span id="pytest-texts-score-api-module"></span><h2>pytest_texts_score.api module<a class="headerlink" href="#module-pytest_texts_score.api" title="Link to this heading">¶</a></h2>
<dl class="py data">
<dt class="sig sig-object py" id="pytest_texts_score.api.MINIMAL_EXPECTED_MAX_DELTA">
<span class="sig-prename descclassname"><span class="pre">pytest_texts_score.api.</span></span><span class="sig-name descname"><span class="pre">MINIMAL_EXPECTED_MAX_DELTA</span></span><span class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">0.05</span></span><a class="headerlink" href="#pytest_texts_score.api.MINIMAL_EXPECTED_MAX_DELTA" title="Link to this definition">¶</a></dt>
<dd><p>A recommended minimum value for the <cite>max_delta</cite> or range width.
Used to warn users if their test’s acceptance criteria are very strict,
which might lead to flaky tests due to LLM non-determinism.</p>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="pytest_texts_score.api.texts_agg_f1_max">
<span class="sig-prename descclassname"><span class="pre">pytest_texts_score.api.</span></span><span class="sig-name descname"><span class="pre">texts_agg_f1_max</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">expected</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">given</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">upper_bound</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">full_runs</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">5</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">each_question_runs</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">retry_on_error</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">None</span></span></span><a class="reference internal" href="_modules/pytest_texts_score/api.html#texts_agg_f1_max"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#pytest_texts_score.api.texts_agg_f1_max" title="Link to this definition">¶</a></dt>
<dd><p>Assert that the maximum aggregated F1 score is below an upper bound.</p>
<p>Performs multiple evaluation runs, calculates the maximum F1 score across
all runs, and asserts that this maximum score is less than or equal to
<code class="docutils literal notranslate"><span class="pre">upper_bound</span></code>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>expected</strong> (<em>str</em>) – The reference text.</p></li>
<li><p><strong>given</strong> (<em>str</em>) – The text to evaluate.</p></li>
<li><p><strong>upper_bound</strong> (<em>float</em>) – The maximum acceptable score for the aggregated maximum.</p></li>
<li><p><strong>full_runs</strong> (<em>int</em>) – Number of times to generate new questions. Defaults to 5.</p></li>
<li><p><strong>each_question_runs</strong> (<em>int</em>) – Number of times to evaluate answers per question set. Defaults to 1.</p></li>
<li><p><strong>retry_on_error</strong> (<em>bool</em>) – If <code class="docutils literal notranslate"><span class="pre">True</span></code>, retries LLM calls on failure.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="pytest_texts_score.api.texts_agg_f1_mean">
<span class="sig-prename descclassname"><span class="pre">pytest_texts_score.api.</span></span><span class="sig-name descname"><span class="pre">texts_agg_f1_mean</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">expected</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">given</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">target</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_delta</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">0.1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">full_runs</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">5</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">each_question_runs</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">retry_on_error</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">None</span></span></span><a class="reference internal" href="_modules/pytest_texts_score/api.html#texts_agg_f1_mean"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#pytest_texts_score.api.texts_agg_f1_mean" title="Link to this definition">¶</a></dt>
<dd><p>Assert that the mean aggregated F1 score is close to a target value.</p>
<p>Performs multiple evaluation runs, calculates the mean (average) F1 score,
and asserts that it falls within the range defined by <code class="docutils literal notranslate"><span class="pre">target</span> <span class="pre">±</span> <span class="pre">max_delta</span></code>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>expected</strong> (<em>str</em>) – The reference text.</p></li>
<li><p><strong>given</strong> (<em>str</em>) – The text to evaluate.</p></li>
<li><p><strong>target</strong> (<em>float</em>) – The expected mean score.</p></li>
<li><p><strong>max_delta</strong> (<em>float</em>) – The allowed deviation from the target. Defaults to 0.1.</p></li>
<li><p><strong>full_runs</strong> (<em>int</em>) – Number of times to generate new questions. Defaults to 5.</p></li>
<li><p><strong>each_question_runs</strong> (<em>int</em>) – Number of times to evaluate answers per question set. Defaults to 1.</p></li>
<li><p><strong>retry_on_error</strong> (<em>bool</em>) – If <code class="docutils literal notranslate"><span class="pre">True</span></code>, retries LLM calls on failure.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="pytest_texts_score.api.texts_agg_f1_median">
<span class="sig-prename descclassname"><span class="pre">pytest_texts_score.api.</span></span><span class="sig-name descname"><span class="pre">texts_agg_f1_median</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">expected</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">given</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">target</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_delta</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">0.1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">full_runs</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">5</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">each_question_runs</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">retry_on_error</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">None</span></span></span><a class="reference internal" href="_modules/pytest_texts_score/api.html#texts_agg_f1_median"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#pytest_texts_score.api.texts_agg_f1_median" title="Link to this definition">¶</a></dt>
<dd><p>Assert that the median aggregated F1 score is close to a target value.</p>
<p>Performs multiple evaluation runs, calculates the median F1 score, and
asserts that it falls within the range defined by <code class="docutils literal notranslate"><span class="pre">target</span> <span class="pre">±</span> <span class="pre">max_delta</span></code>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>expected</strong> (<em>str</em>) – The reference text.</p></li>
<li><p><strong>given</strong> (<em>str</em>) – The text to evaluate.</p></li>
<li><p><strong>target</strong> (<em>float</em>) – The expected median score.</p></li>
<li><p><strong>max_delta</strong> (<em>float</em>) – The allowed deviation from the target. Defaults to 0.1.</p></li>
<li><p><strong>full_runs</strong> (<em>int</em>) – Number of times to generate new questions. Defaults to 5.</p></li>
<li><p><strong>each_question_runs</strong> (<em>int</em>) – Number of times to evaluate answers per question set. Defaults to 1.</p></li>
<li><p><strong>retry_on_error</strong> (<em>bool</em>) – If <code class="docutils literal notranslate"><span class="pre">True</span></code>, retries LLM calls on failure.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="pytest_texts_score.api.texts_agg_f1_min">
<span class="sig-prename descclassname"><span class="pre">pytest_texts_score.api.</span></span><span class="sig-name descname"><span class="pre">texts_agg_f1_min</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">expected</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">given</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">lower_bound</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">full_runs</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">5</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">each_question_runs</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">retry_on_error</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">None</span></span></span><a class="reference internal" href="_modules/pytest_texts_score/api.html#texts_agg_f1_min"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#pytest_texts_score.api.texts_agg_f1_min" title="Link to this definition">¶</a></dt>
<dd><p>Assert that the minimum aggregated F1 score is above a lower bound.</p>
<p>Performs multiple evaluation runs, calculates the minimum F1 score across
all runs, and asserts that this minimum score is greater than or equal to
<code class="docutils literal notranslate"><span class="pre">lower_bound</span></code>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>expected</strong> (<em>str</em>) – The reference text.</p></li>
<li><p><strong>given</strong> (<em>str</em>) – The text to evaluate.</p></li>
<li><p><strong>lower_bound</strong> (<em>float</em>) – The minimum acceptable score for the aggregated minimum.</p></li>
<li><p><strong>full_runs</strong> (<em>int</em>) – Number of times to generate new questions. Defaults to 5.</p></li>
<li><p><strong>each_question_runs</strong> (<em>int</em>) – Number of times to evaluate answers per question set. Defaults to 1.</p></li>
<li><p><strong>retry_on_error</strong> (<em>bool</em>) – If <code class="docutils literal notranslate"><span class="pre">True</span></code>, retries LLM calls on failure.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="pytest_texts_score.api.texts_agg_precision_max">
<span class="sig-prename descclassname"><span class="pre">pytest_texts_score.api.</span></span><span class="sig-name descname"><span class="pre">texts_agg_precision_max</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">expected</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">given</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">upper_bound</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">full_runs</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">5</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">each_question_runs</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">retry_on_error</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">None</span></span></span><a class="reference internal" href="_modules/pytest_texts_score/api.html#texts_agg_precision_max"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#pytest_texts_score.api.texts_agg_precision_max" title="Link to this definition">¶</a></dt>
<dd><p>Assert that the maximum aggregated precision is below an upper bound.</p>
<p>Performs multiple evaluation runs, calculates the maximum precision score
across all runs, and asserts that this maximum score is less than or equal
to <code class="docutils literal notranslate"><span class="pre">upper_bound</span></code>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>expected</strong> (<em>str</em>) – The reference text.</p></li>
<li><p><strong>given</strong> (<em>str</em>) – The text to evaluate.</p></li>
<li><p><strong>upper_bound</strong> (<em>float</em>) – The maximum acceptable score for the aggregated maximum.</p></li>
<li><p><strong>full_runs</strong> (<em>int</em>) – Number of times to generate new questions. Defaults to 5.</p></li>
<li><p><strong>each_question_runs</strong> (<em>int</em>) – Number of times to evaluate answers per question set. Defaults to 1.</p></li>
<li><p><strong>retry_on_error</strong> (<em>bool</em>) – If <code class="docutils literal notranslate"><span class="pre">True</span></code>, retries LLM calls on failure.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="pytest_texts_score.api.texts_agg_precision_mean">
<span class="sig-prename descclassname"><span class="pre">pytest_texts_score.api.</span></span><span class="sig-name descname"><span class="pre">texts_agg_precision_mean</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">expected</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">given</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">target</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_delta</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">0.1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">full_runs</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">5</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">each_question_runs</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">retry_on_error</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">None</span></span></span><a class="reference internal" href="_modules/pytest_texts_score/api.html#texts_agg_precision_mean"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#pytest_texts_score.api.texts_agg_precision_mean" title="Link to this definition">¶</a></dt>
<dd><p>Assert that the mean aggregated precision is close to a target value.</p>
<p>Performs multiple evaluation runs, calculates the mean (average) precision
score, and asserts that it falls within the range <code class="docutils literal notranslate"><span class="pre">target</span> <span class="pre">±</span> <span class="pre">max_delta</span></code>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>expected</strong> (<em>str</em>) – The reference text.</p></li>
<li><p><strong>given</strong> (<em>str</em>) – The text to evaluate.</p></li>
<li><p><strong>target</strong> (<em>float</em>) – The expected mean score.</p></li>
<li><p><strong>max_delta</strong> (<em>float</em>) – The allowed deviation from the target. Defaults to 0.1.</p></li>
<li><p><strong>full_runs</strong> (<em>int</em>) – Number of times to generate new questions. Defaults to 5.</p></li>
<li><p><strong>each_question_runs</strong> (<em>int</em>) – Number of times to evaluate answers per question set. Defaults to 1.</p></li>
<li><p><strong>retry_on_error</strong> (<em>bool</em>) – If <code class="docutils literal notranslate"><span class="pre">True</span></code>, retries LLM calls on failure.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="pytest_texts_score.api.texts_agg_precision_median">
<span class="sig-prename descclassname"><span class="pre">pytest_texts_score.api.</span></span><span class="sig-name descname"><span class="pre">texts_agg_precision_median</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">expected</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">given</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">target</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_delta</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">0.1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">full_runs</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">5</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">each_question_runs</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">retry_on_error</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">None</span></span></span><a class="reference internal" href="_modules/pytest_texts_score/api.html#texts_agg_precision_median"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#pytest_texts_score.api.texts_agg_precision_median" title="Link to this definition">¶</a></dt>
<dd><p>Assert that the median aggregated precision is close to a target value.</p>
<p>Performs multiple evaluation runs, calculates the median precision score,
and asserts that it falls within the range defined by <code class="docutils literal notranslate"><span class="pre">target</span> <span class="pre">±</span> <span class="pre">max_delta</span></code>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>expected</strong> (<em>str</em>) – The reference text.</p></li>
<li><p><strong>given</strong> (<em>str</em>) – The text to evaluate.</p></li>
<li><p><strong>target</strong> (<em>float</em>) – The expected median score.</p></li>
<li><p><strong>max_delta</strong> (<em>float</em>) – The allowed deviation from the target. Defaults to 0.1.</p></li>
<li><p><strong>full_runs</strong> (<em>int</em>) – Number of times to generate new questions. Defaults to 5.</p></li>
<li><p><strong>each_question_runs</strong> (<em>int</em>) – Number of times to evaluate answers per question set. Defaults to 1.</p></li>
<li><p><strong>retry_on_error</strong> (<em>bool</em>) – If <code class="docutils literal notranslate"><span class="pre">True</span></code>, retries LLM calls on failure.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="pytest_texts_score.api.texts_agg_precision_min">
<span class="sig-prename descclassname"><span class="pre">pytest_texts_score.api.</span></span><span class="sig-name descname"><span class="pre">texts_agg_precision_min</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">expected</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">given</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">lower_bound</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">full_runs</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">5</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">each_question_runs</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">retry_on_error</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">None</span></span></span><a class="reference internal" href="_modules/pytest_texts_score/api.html#texts_agg_precision_min"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#pytest_texts_score.api.texts_agg_precision_min" title="Link to this definition">¶</a></dt>
<dd><p>Assert that the minimum aggregated precision is above a lower bound.</p>
<p>Performs multiple evaluation runs, calculates the minimum precision score
across all runs, and asserts that this minimum score is greater than or
equal to <code class="docutils literal notranslate"><span class="pre">lower_bound</span></code>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>expected</strong> (<em>str</em>) – The reference text.</p></li>
<li><p><strong>given</strong> (<em>str</em>) – The text to evaluate.</p></li>
<li><p><strong>lower_bound</strong> (<em>float</em>) – The minimum acceptable score for the aggregated minimum.</p></li>
<li><p><strong>full_runs</strong> (<em>int</em>) – Number of times to generate new questions. Defaults to 5.</p></li>
<li><p><strong>each_question_runs</strong> (<em>int</em>) – Number of times to evaluate answers per question set. Defaults to 1.</p></li>
<li><p><strong>retry_on_error</strong> (<em>bool</em>) – If <code class="docutils literal notranslate"><span class="pre">True</span></code>, retries LLM calls on failure.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="pytest_texts_score.api.texts_agg_recall_max">
<span class="sig-prename descclassname"><span class="pre">pytest_texts_score.api.</span></span><span class="sig-name descname"><span class="pre">texts_agg_recall_max</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">expected</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">given</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">upper_bound</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">full_runs</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">5</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">each_question_runs</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">retry_on_error</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">None</span></span></span><a class="reference internal" href="_modules/pytest_texts_score/api.html#texts_agg_recall_max"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#pytest_texts_score.api.texts_agg_recall_max" title="Link to this definition">¶</a></dt>
<dd><p>Assert that the maximum aggregated recall is below an upper bound.</p>
<p>Performs multiple evaluation runs, calculates the maximum recall score
across all runs, and asserts that this maximum score is less than or equal
to <code class="docutils literal notranslate"><span class="pre">upper_bound</span></code>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>expected</strong> (<em>str</em>) – The reference text.</p></li>
<li><p><strong>given</strong> (<em>str</em>) – The text to evaluate.</p></li>
<li><p><strong>upper_bound</strong> (<em>float</em>) – The maximum acceptable score for the aggregated maximum.</p></li>
<li><p><strong>full_runs</strong> (<em>int</em>) – Number of times to generate new questions. Defaults to 5.</p></li>
<li><p><strong>each_question_runs</strong> (<em>int</em>) – Number of times to evaluate answers per question set. Defaults to 1.</p></li>
<li><p><strong>retry_on_error</strong> (<em>bool</em>) – If <code class="docutils literal notranslate"><span class="pre">True</span></code>, retries LLM calls on failure.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="pytest_texts_score.api.texts_agg_recall_mean">
<span class="sig-prename descclassname"><span class="pre">pytest_texts_score.api.</span></span><span class="sig-name descname"><span class="pre">texts_agg_recall_mean</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">expected</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">given</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">target</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_delta</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">0.1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">full_runs</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">5</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">each_question_runs</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">retry_on_error</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">None</span></span></span><a class="reference internal" href="_modules/pytest_texts_score/api.html#texts_agg_recall_mean"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#pytest_texts_score.api.texts_agg_recall_mean" title="Link to this definition">¶</a></dt>
<dd><p>Assert that the mean aggregated recall is close to a target value.</p>
<p>Performs multiple evaluation runs, calculates the mean (average) recall
score, and asserts that it falls within the range <code class="docutils literal notranslate"><span class="pre">target</span> <span class="pre">±</span> <span class="pre">max_delta</span></code>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>expected</strong> (<em>str</em>) – The reference text.</p></li>
<li><p><strong>given</strong> (<em>str</em>) – The text to evaluate.</p></li>
<li><p><strong>target</strong> (<em>float</em>) – The expected mean score.</p></li>
<li><p><strong>max_delta</strong> (<em>float</em>) – The allowed deviation from the target. Defaults to 0.1.</p></li>
<li><p><strong>full_runs</strong> (<em>int</em>) – Number of times to generate new questions. Defaults to 5.</p></li>
<li><p><strong>each_question_runs</strong> (<em>int</em>) – Number of times to evaluate answers per question set. Defaults to 1.</p></li>
<li><p><strong>retry_on_error</strong> (<em>bool</em>) – If <code class="docutils literal notranslate"><span class="pre">True</span></code>, retries LLM calls on failure.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="pytest_texts_score.api.texts_agg_recall_median">
<span class="sig-prename descclassname"><span class="pre">pytest_texts_score.api.</span></span><span class="sig-name descname"><span class="pre">texts_agg_recall_median</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">expected</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">given</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">target</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_delta</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">0.1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">full_runs</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">5</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">each_question_runs</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">retry_on_error</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">None</span></span></span><a class="reference internal" href="_modules/pytest_texts_score/api.html#texts_agg_recall_median"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#pytest_texts_score.api.texts_agg_recall_median" title="Link to this definition">¶</a></dt>
<dd><p>Assert that the median aggregated recall is close to a target value.</p>
<p>Performs multiple evaluation runs, calculates the median recall score, and
asserts that it falls within the range defined by <code class="docutils literal notranslate"><span class="pre">target</span> <span class="pre">±</span> <span class="pre">max_delta</span></code>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>expected</strong> (<em>str</em>) – The reference text.</p></li>
<li><p><strong>given</strong> (<em>str</em>) – The text to evaluate.</p></li>
<li><p><strong>target</strong> (<em>float</em>) – The expected median score.</p></li>
<li><p><strong>max_delta</strong> (<em>float</em>) – The allowed deviation from the target. Defaults to 0.1.</p></li>
<li><p><strong>full_runs</strong> (<em>int</em>) – Number of times to generate new questions. Defaults to 5.</p></li>
<li><p><strong>each_question_runs</strong> (<em>int</em>) – Number of times to evaluate answers per question set. Defaults to 1.</p></li>
<li><p><strong>retry_on_error</strong> (<em>bool</em>) – If <code class="docutils literal notranslate"><span class="pre">True</span></code>, retries LLM calls on failure.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="pytest_texts_score.api.texts_agg_recall_min">
<span class="sig-prename descclassname"><span class="pre">pytest_texts_score.api.</span></span><span class="sig-name descname"><span class="pre">texts_agg_recall_min</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">expected</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">given</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">lower_bound</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">full_runs</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">5</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">each_question_runs</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">retry_on_error</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">None</span></span></span><a class="reference internal" href="_modules/pytest_texts_score/api.html#texts_agg_recall_min"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#pytest_texts_score.api.texts_agg_recall_min" title="Link to this definition">¶</a></dt>
<dd><p>Assert that the minimum aggregated recall is above a lower bound.</p>
<p>Performs multiple evaluation runs, calculates the minimum recall score
across all runs, and asserts that this minimum score is greater than or
equal to <code class="docutils literal notranslate"><span class="pre">lower_bound</span></code>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>expected</strong> (<em>str</em>) – The reference text.</p></li>
<li><p><strong>given</strong> (<em>str</em>) – The text to evaluate.</p></li>
<li><p><strong>lower_bound</strong> (<em>float</em>) – The minimum acceptable score for the aggregated minimum.</p></li>
<li><p><strong>full_runs</strong> (<em>int</em>) – Number of times to generate new questions. Defaults to 5.</p></li>
<li><p><strong>each_question_runs</strong> (<em>int</em>) – Number of times to evaluate answers per question set. Defaults to 1.</p></li>
<li><p><strong>retry_on_error</strong> (<em>bool</em>) – If <code class="docutils literal notranslate"><span class="pre">True</span></code>, retries LLM calls on failure.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="pytest_texts_score.api.texts_expect_f1_equal">
<span class="sig-prename descclassname"><span class="pre">pytest_texts_score.api.</span></span><span class="sig-name descname"><span class="pre">texts_expect_f1_equal</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">expected</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">given</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">target</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">1.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_delta</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">0.2</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">skip_warnings</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">retry_on_error</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">None</span></span></span><a class="reference internal" href="_modules/pytest_texts_score/api.html#texts_expect_f1_equal"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#pytest_texts_score.api.texts_expect_f1_equal" title="Link to this definition">¶</a></dt>
<dd><p>Assert that the F1 score is close to a target value.</p>
<p>This is a convenience wrapper around <a class="reference internal" href="#pytest_texts_score.api.texts_expect_f1_range" title="pytest_texts_score.api.texts_expect_f1_range"><code class="xref py py-func docutils literal notranslate"><span class="pre">texts_expect_f1_range()</span></code></a>.
It performs a single F1 score evaluation and asserts that the result
is within <code class="docutils literal notranslate"><span class="pre">target</span> <span class="pre">±</span> <span class="pre">max_delta</span></code>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>expected</strong> (<em>str</em>) – The reference text.</p></li>
<li><p><strong>given</strong> (<em>str</em>) – The text to evaluate.</p></li>
<li><p><strong>target</strong> (<em>float</em>) – The expected F1 score. Defaults to 1.0.</p></li>
<li><p><strong>max_delta</strong> (<em>float</em>) – The allowed deviation from the target. Defaults to 0.2.</p></li>
<li><p><strong>skip_warnings</strong> (<em>bool</em>) – If <code class="docutils literal notranslate"><span class="pre">True</span></code>, suppresses input validation warnings.</p></li>
<li><p><strong>retry_on_error</strong> (<em>bool</em>) – If <code class="docutils literal notranslate"><span class="pre">True</span></code>, retries LLM calls on failure.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="pytest_texts_score.api.texts_expect_f1_range">
<span class="sig-prename descclassname"><span class="pre">pytest_texts_score.api.</span></span><span class="sig-name descname"><span class="pre">texts_expect_f1_range</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">expected</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">given</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">min_score</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_score</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">skip_warnings</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">retry_on_error</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">None</span></span></span><a class="reference internal" href="_modules/pytest_texts_score/api.html#texts_expect_f1_range"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#pytest_texts_score.api.texts_expect_f1_range" title="Link to this definition">¶</a></dt>
<dd><p>Assert that the F1 score falls within a specified range.</p>
<p>This function performs a single evaluation of the F1 score between the
<code class="docutils literal notranslate"><span class="pre">expected</span></code> and <code class="docutils literal notranslate"><span class="pre">given</span></code> texts. It then asserts that the resulting score
is between <code class="docutils literal notranslate"><span class="pre">min_score</span></code> and <code class="docutils literal notranslate"><span class="pre">max_score</span></code>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>expected</strong> (<em>str</em>) – The reference text.</p></li>
<li><p><strong>given</strong> (<em>str</em>) – The text to evaluate.</p></li>
<li><p><strong>min_score</strong> (<em>float</em>) – The minimum acceptable F1 score.</p></li>
<li><p><strong>max_score</strong> (<em>float</em>) – The maximum acceptable F1 score.</p></li>
<li><p><strong>skip_warnings</strong> (<em>bool</em>) – If <code class="docutils literal notranslate"><span class="pre">True</span></code>, suppresses input validation warnings.</p></li>
<li><p><strong>retry_on_error</strong> (<em>bool</em>) – If <code class="docutils literal notranslate"><span class="pre">True</span></code>, retries LLM calls on failure.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="pytest_texts_score.api.texts_expect_precision_equal">
<span class="sig-prename descclassname"><span class="pre">pytest_texts_score.api.</span></span><span class="sig-name descname"><span class="pre">texts_expect_precision_equal</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">expected</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">given</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">target</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">1.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_delta</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">0.2</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">skip_warnings</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">retry_on_error</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">None</span></span></span><a class="reference internal" href="_modules/pytest_texts_score/api.html#texts_expect_precision_equal"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#pytest_texts_score.api.texts_expect_precision_equal" title="Link to this definition">¶</a></dt>
<dd><p>Assert that the precision score is close to a target value.</p>
<p>This is a convenience wrapper around <a class="reference internal" href="#pytest_texts_score.api.texts_expect_precision_range" title="pytest_texts_score.api.texts_expect_precision_range"><code class="xref py py-func docutils literal notranslate"><span class="pre">texts_expect_precision_range()</span></code></a>.
It performs a single precision score evaluation and asserts that the result
is within <code class="docutils literal notranslate"><span class="pre">target</span> <span class="pre">±</span> <span class="pre">max_delta</span></code>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>expected</strong> (<em>str</em>) – The reference text.</p></li>
<li><p><strong>given</strong> (<em>str</em>) – The text to evaluate.</p></li>
<li><p><strong>target</strong> (<em>float</em>) – The expected precision score. Defaults to 1.0.</p></li>
<li><p><strong>max_delta</strong> (<em>float</em>) – The allowed deviation from the target. Defaults to 0.2.</p></li>
<li><p><strong>skip_warnings</strong> (<em>bool</em>) – If <code class="docutils literal notranslate"><span class="pre">True</span></code>, suppresses input validation warnings.</p></li>
<li><p><strong>retry_on_error</strong> (<em>bool</em>) – If <code class="docutils literal notranslate"><span class="pre">True</span></code>, retries LLM calls on failure.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="pytest_texts_score.api.texts_expect_precision_range">
<span class="sig-prename descclassname"><span class="pre">pytest_texts_score.api.</span></span><span class="sig-name descname"><span class="pre">texts_expect_precision_range</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">expected</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">given</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">min_score</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_score</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">skip_warnings</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">retry_on_error</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">None</span></span></span><a class="reference internal" href="_modules/pytest_texts_score/api.html#texts_expect_precision_range"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#pytest_texts_score.api.texts_expect_precision_range" title="Link to this definition">¶</a></dt>
<dd><p>Assert that the precision score falls within a specified range.</p>
<p>This function performs a single evaluation of the precision score between the
<code class="docutils literal notranslate"><span class="pre">expected</span></code> and <code class="docutils literal notranslate"><span class="pre">given</span></code> texts. It then asserts that the resulting score
is between <code class="docutils literal notranslate"><span class="pre">min_score</span></code> and <code class="docutils literal notranslate"><span class="pre">max_score</span></code>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>expected</strong> (<em>str</em>) – The reference text.</p></li>
<li><p><strong>given</strong> (<em>str</em>) – The text to evaluate.</p></li>
<li><p><strong>min_score</strong> (<em>float</em>) – The minimum acceptable precision score.</p></li>
<li><p><strong>max_score</strong> (<em>float</em>) – The maximum acceptable precision score.</p></li>
<li><p><strong>skip_warnings</strong> (<em>bool</em>) – If <code class="docutils literal notranslate"><span class="pre">True</span></code>, suppresses input validation warnings.</p></li>
<li><p><strong>retry_on_error</strong> (<em>bool</em>) – If <code class="docutils literal notranslate"><span class="pre">True</span></code>, retries LLM calls on failure.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="pytest_texts_score.api.texts_expect_recall_equal">
<span class="sig-prename descclassname"><span class="pre">pytest_texts_score.api.</span></span><span class="sig-name descname"><span class="pre">texts_expect_recall_equal</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">expected</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">given</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">target</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">1.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_delta</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">0.2</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">skip_warnings</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">retry_on_error</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">None</span></span></span><a class="reference internal" href="_modules/pytest_texts_score/api.html#texts_expect_recall_equal"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#pytest_texts_score.api.texts_expect_recall_equal" title="Link to this definition">¶</a></dt>
<dd><p>Assert that the recall score is close to a target value.</p>
<p>This is a convenience wrapper around <a class="reference internal" href="#pytest_texts_score.api.texts_expect_recall_range" title="pytest_texts_score.api.texts_expect_recall_range"><code class="xref py py-func docutils literal notranslate"><span class="pre">texts_expect_recall_range()</span></code></a>.
It performs a single recall score evaluation and asserts that the result
is within <code class="docutils literal notranslate"><span class="pre">target</span> <span class="pre">±</span> <span class="pre">max_delta</span></code>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>expected</strong> (<em>str</em>) – The reference text.</p></li>
<li><p><strong>given</strong> (<em>str</em>) – The text to evaluate.</p></li>
<li><p><strong>target</strong> (<em>float</em>) – The expected recall score. Defaults to 1.0.</p></li>
<li><p><strong>max_delta</strong> (<em>float</em>) – The allowed deviation from the target. Defaults to 0.2.</p></li>
<li><p><strong>skip_warnings</strong> (<em>bool</em>) – If <code class="docutils literal notranslate"><span class="pre">True</span></code>, suppresses input validation warnings.</p></li>
<li><p><strong>retry_on_error</strong> (<em>bool</em>) – If <code class="docutils literal notranslate"><span class="pre">True</span></code>, retries LLM calls on failure.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="pytest_texts_score.api.texts_expect_recall_range">
<span class="sig-prename descclassname"><span class="pre">pytest_texts_score.api.</span></span><span class="sig-name descname"><span class="pre">texts_expect_recall_range</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">expected</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">given</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">min_score</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_score</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">skip_warnings</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">retry_on_error</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">None</span></span></span><a class="reference internal" href="_modules/pytest_texts_score/api.html#texts_expect_recall_range"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#pytest_texts_score.api.texts_expect_recall_range" title="Link to this definition">¶</a></dt>
<dd><p>Assert that the recall score falls within a specified range.</p>
<p>This function performs a single evaluation of the recall score between the
<code class="docutils literal notranslate"><span class="pre">expected</span></code> and <code class="docutils literal notranslate"><span class="pre">given</span></code> texts. It then asserts that the resulting score
is between <code class="docutils literal notranslate"><span class="pre">min_score</span></code> and <code class="docutils literal notranslate"><span class="pre">max_score</span></code>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>expected</strong> (<em>str</em>) – The reference text.</p></li>
<li><p><strong>given</strong> (<em>str</em>) – The text to evaluate.</p></li>
<li><p><strong>min_score</strong> (<em>float</em>) – The minimum acceptable recall score.</p></li>
<li><p><strong>max_score</strong> (<em>float</em>) – The maximum acceptable recall score.</p></li>
<li><p><strong>skip_warnings</strong> (<em>bool</em>) – If <code class="docutils literal notranslate"><span class="pre">True</span></code>, suppresses input validation warnings.</p></li>
<li><p><strong>retry_on_error</strong> (<em>bool</em>) – If <code class="docutils literal notranslate"><span class="pre">True</span></code>, retries LLM calls on failure.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

</section>
<section id="module-pytest_texts_score.api_wrappers">
<span id="pytest-texts-score-api-wrappers-module"></span><h2>pytest_texts_score.api_wrappers module<a class="headerlink" href="#module-pytest_texts_score.api_wrappers" title="Link to this heading">¶</a></h2>
<p>This module provides wrapper functions for the public API, offering alternative
names for existing functionality. For example, functions using ‘mean’ are
aliased with ‘average’.</p>
<dl class="py function">
<dt class="sig sig-object py" id="pytest_texts_score.api_wrappers.texts_agg_completeness_average">
<span class="sig-prename descclassname"><span class="pre">pytest_texts_score.api_wrappers.</span></span><span class="sig-name descname"><span class="pre">texts_agg_completeness_average</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">expected</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">given</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">target</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_delta</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">0.1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">full_runs</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">5</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">each_question_runs</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">retry_on_error</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">None</span></span></span><a class="reference internal" href="_modules/pytest_texts_score/api_wrappers.html#texts_agg_completeness_average"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#pytest_texts_score.api_wrappers.texts_agg_completeness_average" title="Link to this definition">¶</a></dt>
<dd><p>Alias for texts_agg_precision_average.</p>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="pytest_texts_score.api_wrappers.texts_agg_completeness_max">
<span class="sig-prename descclassname"><span class="pre">pytest_texts_score.api_wrappers.</span></span><span class="sig-name descname"><span class="pre">texts_agg_completeness_max</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">expected</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">given</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">upper_bound</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">full_runs</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">5</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">each_question_runs</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">retry_on_error</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">None</span></span></span><a class="reference internal" href="_modules/pytest_texts_score/api_wrappers.html#texts_agg_completeness_max"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#pytest_texts_score.api_wrappers.texts_agg_completeness_max" title="Link to this definition">¶</a></dt>
<dd><p>Alias for texts_agg_precision_max.</p>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="pytest_texts_score.api_wrappers.texts_agg_completeness_mean">
<span class="sig-prename descclassname"><span class="pre">pytest_texts_score.api_wrappers.</span></span><span class="sig-name descname"><span class="pre">texts_agg_completeness_mean</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">expected</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">given</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">target</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_delta</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">0.1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">full_runs</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">5</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">each_question_runs</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">retry_on_error</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">None</span></span></span><a class="reference internal" href="_modules/pytest_texts_score/api_wrappers.html#texts_agg_completeness_mean"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#pytest_texts_score.api_wrappers.texts_agg_completeness_mean" title="Link to this definition">¶</a></dt>
<dd><p>Alias for texts_agg_precision_mean.</p>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="pytest_texts_score.api_wrappers.texts_agg_completeness_median">
<span class="sig-prename descclassname"><span class="pre">pytest_texts_score.api_wrappers.</span></span><span class="sig-name descname"><span class="pre">texts_agg_completeness_median</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">expected</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">given</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">target</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_delta</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">0.1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">full_runs</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">5</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">each_question_runs</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">retry_on_error</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">None</span></span></span><a class="reference internal" href="_modules/pytest_texts_score/api_wrappers.html#texts_agg_completeness_median"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#pytest_texts_score.api_wrappers.texts_agg_completeness_median" title="Link to this definition">¶</a></dt>
<dd><p>Alias for texts_agg_precision_median.</p>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="pytest_texts_score.api_wrappers.texts_agg_completeness_min">
<span class="sig-prename descclassname"><span class="pre">pytest_texts_score.api_wrappers.</span></span><span class="sig-name descname"><span class="pre">texts_agg_completeness_min</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">expected</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">given</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">lower_bound</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">full_runs</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">5</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">each_question_runs</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">retry_on_error</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">None</span></span></span><a class="reference internal" href="_modules/pytest_texts_score/api_wrappers.html#texts_agg_completeness_min"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#pytest_texts_score.api_wrappers.texts_agg_completeness_min" title="Link to this definition">¶</a></dt>
<dd><p>Alias for texts_agg_precision_min.</p>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="pytest_texts_score.api_wrappers.texts_agg_correctness_average">
<span class="sig-prename descclassname"><span class="pre">pytest_texts_score.api_wrappers.</span></span><span class="sig-name descname"><span class="pre">texts_agg_correctness_average</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">expected</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">given</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">target</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_delta</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">0.1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">full_runs</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">5</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">each_question_runs</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">retry_on_error</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">None</span></span></span><a class="reference internal" href="_modules/pytest_texts_score/api_wrappers.html#texts_agg_correctness_average"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#pytest_texts_score.api_wrappers.texts_agg_correctness_average" title="Link to this definition">¶</a></dt>
<dd><p>Alias for texts_agg_recall_average.</p>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="pytest_texts_score.api_wrappers.texts_agg_correctness_max">
<span class="sig-prename descclassname"><span class="pre">pytest_texts_score.api_wrappers.</span></span><span class="sig-name descname"><span class="pre">texts_agg_correctness_max</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">expected</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">given</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">upper_bound</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">full_runs</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">5</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">each_question_runs</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">retry_on_error</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">None</span></span></span><a class="reference internal" href="_modules/pytest_texts_score/api_wrappers.html#texts_agg_correctness_max"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#pytest_texts_score.api_wrappers.texts_agg_correctness_max" title="Link to this definition">¶</a></dt>
<dd><p>Alias for texts_agg_recall_max.</p>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="pytest_texts_score.api_wrappers.texts_agg_correctness_mean">
<span class="sig-prename descclassname"><span class="pre">pytest_texts_score.api_wrappers.</span></span><span class="sig-name descname"><span class="pre">texts_agg_correctness_mean</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">expected</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">given</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">target</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_delta</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">0.1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">full_runs</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">5</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">each_question_runs</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">retry_on_error</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">None</span></span></span><a class="reference internal" href="_modules/pytest_texts_score/api_wrappers.html#texts_agg_correctness_mean"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#pytest_texts_score.api_wrappers.texts_agg_correctness_mean" title="Link to this definition">¶</a></dt>
<dd><p>Alias for texts_agg_recall_mean.</p>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="pytest_texts_score.api_wrappers.texts_agg_correctness_median">
<span class="sig-prename descclassname"><span class="pre">pytest_texts_score.api_wrappers.</span></span><span class="sig-name descname"><span class="pre">texts_agg_correctness_median</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">expected</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">given</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">target</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_delta</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">0.1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">full_runs</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">5</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">each_question_runs</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">retry_on_error</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">None</span></span></span><a class="reference internal" href="_modules/pytest_texts_score/api_wrappers.html#texts_agg_correctness_median"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#pytest_texts_score.api_wrappers.texts_agg_correctness_median" title="Link to this definition">¶</a></dt>
<dd><p>Alias for texts_agg_recall_median.</p>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="pytest_texts_score.api_wrappers.texts_agg_correctness_min">
<span class="sig-prename descclassname"><span class="pre">pytest_texts_score.api_wrappers.</span></span><span class="sig-name descname"><span class="pre">texts_agg_correctness_min</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">expected</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">given</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">lower_bound</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">full_runs</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">5</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">each_question_runs</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">retry_on_error</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">None</span></span></span><a class="reference internal" href="_modules/pytest_texts_score/api_wrappers.html#texts_agg_correctness_min"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#pytest_texts_score.api_wrappers.texts_agg_correctness_min" title="Link to this definition">¶</a></dt>
<dd><p>Alias for texts_agg_recall_min.</p>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="pytest_texts_score.api_wrappers.texts_agg_f1_average">
<span class="sig-prename descclassname"><span class="pre">pytest_texts_score.api_wrappers.</span></span><span class="sig-name descname"><span class="pre">texts_agg_f1_average</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">expected</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">given</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">target</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_delta</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">0.1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">full_runs</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">5</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">each_question_runs</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">retry_on_error</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">None</span></span></span><a class="reference internal" href="_modules/pytest_texts_score/api_wrappers.html#texts_agg_f1_average"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#pytest_texts_score.api_wrappers.texts_agg_f1_average" title="Link to this definition">¶</a></dt>
<dd><p>Alias for texts_agg_f1_mean.</p>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="pytest_texts_score.api_wrappers.texts_agg_precision_average">
<span class="sig-prename descclassname"><span class="pre">pytest_texts_score.api_wrappers.</span></span><span class="sig-name descname"><span class="pre">texts_agg_precision_average</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">expected</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">given</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">target</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_delta</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">0.1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">full_runs</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">5</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">each_question_runs</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">retry_on_error</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">None</span></span></span><a class="reference internal" href="_modules/pytest_texts_score/api_wrappers.html#texts_agg_precision_average"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#pytest_texts_score.api_wrappers.texts_agg_precision_average" title="Link to this definition">¶</a></dt>
<dd><p>Alias for texts_agg_precision_mean.</p>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="pytest_texts_score.api_wrappers.texts_agg_recall_average">
<span class="sig-prename descclassname"><span class="pre">pytest_texts_score.api_wrappers.</span></span><span class="sig-name descname"><span class="pre">texts_agg_recall_average</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">expected</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">given</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">target</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_delta</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">0.1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">full_runs</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">5</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">each_question_runs</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">retry_on_error</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">None</span></span></span><a class="reference internal" href="_modules/pytest_texts_score/api_wrappers.html#texts_agg_recall_average"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#pytest_texts_score.api_wrappers.texts_agg_recall_average" title="Link to this definition">¶</a></dt>
<dd><p>Alias for texts_agg_recall_mean.</p>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="pytest_texts_score.api_wrappers.texts_expect_completeness_equal">
<span class="sig-prename descclassname"><span class="pre">pytest_texts_score.api_wrappers.</span></span><span class="sig-name descname"><span class="pre">texts_expect_completeness_equal</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">expected</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">given</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">target</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">1.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_delta</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">0.2</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">skip_warnings</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">retry_on_error</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">None</span></span></span><a class="reference internal" href="_modules/pytest_texts_score/api_wrappers.html#texts_expect_completeness_equal"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#pytest_texts_score.api_wrappers.texts_expect_completeness_equal" title="Link to this definition">¶</a></dt>
<dd><p>Alias for texts_expect_precision_equal.</p>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="pytest_texts_score.api_wrappers.texts_expect_completeness_range">
<span class="sig-prename descclassname"><span class="pre">pytest_texts_score.api_wrappers.</span></span><span class="sig-name descname"><span class="pre">texts_expect_completeness_range</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">expected</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">given</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">min_score</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_score</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">skip_warnings</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">retry_on_error</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">None</span></span></span><a class="reference internal" href="_modules/pytest_texts_score/api_wrappers.html#texts_expect_completeness_range"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#pytest_texts_score.api_wrappers.texts_expect_completeness_range" title="Link to this definition">¶</a></dt>
<dd><p>Alias for texts_expect_precision_range.</p>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="pytest_texts_score.api_wrappers.texts_expect_correctness_equal">
<span class="sig-prename descclassname"><span class="pre">pytest_texts_score.api_wrappers.</span></span><span class="sig-name descname"><span class="pre">texts_expect_correctness_equal</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">expected</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">given</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">target</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">1.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_delta</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">0.2</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">skip_warnings</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">retry_on_error</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">None</span></span></span><a class="reference internal" href="_modules/pytest_texts_score/api_wrappers.html#texts_expect_correctness_equal"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#pytest_texts_score.api_wrappers.texts_expect_correctness_equal" title="Link to this definition">¶</a></dt>
<dd><p>Alias for texts_expect_recall_equal.</p>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="pytest_texts_score.api_wrappers.texts_expect_correctness_range">
<span class="sig-prename descclassname"><span class="pre">pytest_texts_score.api_wrappers.</span></span><span class="sig-name descname"><span class="pre">texts_expect_correctness_range</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">expected</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">given</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">min_score</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_score</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">skip_warnings</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">retry_on_error</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">None</span></span></span><a class="reference internal" href="_modules/pytest_texts_score/api_wrappers.html#texts_expect_correctness_range"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#pytest_texts_score.api_wrappers.texts_expect_correctness_range" title="Link to this definition">¶</a></dt>
<dd><p>Alias for texts_expect_recall_range.</p>
</dd></dl>

</section>
<section id="module-pytest_texts_score.client">
<span id="pytest-texts-score-client-module"></span><h2>pytest_texts_score.client module<a class="headerlink" href="#module-pytest_texts_score.client" title="Link to this heading">¶</a></h2>
<dl class="py function">
<dt class="sig sig-object py" id="pytest_texts_score.client.get_client">
<span class="sig-prename descclassname"><span class="pre">pytest_texts_score.client.</span></span><span class="sig-name descname"><span class="pre">get_client</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">AzureOpenAI</span></span></span><a class="reference internal" href="_modules/pytest_texts_score/client.html#get_client"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#pytest_texts_score.client.get_client" title="Link to this definition">¶</a></dt>
<dd><p>Return the initialized AzureOpenAI client.</p>
<p>Retrieves the globally stored <code class="docutils literal notranslate"><span class="pre">AzureOpenAI</span></code> client instance. It is
designed to be called after <code class="docutils literal notranslate"><span class="pre">init_client()</span></code> has been executed,
typically within a pytest fixture.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns<span class="colon">:</span></dt>
<dd class="field-odd"><p>The initialized <code class="docutils literal notranslate"><span class="pre">AzureOpenAI</span></code> client instance.</p>
</dd>
<dt class="field-even">Return type<span class="colon">:</span></dt>
<dd class="field-even"><p>AzureOpenAI</p>
</dd>
<dt class="field-odd">Raises<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>RuntimeError</strong> – If the client has not been initialized by calling <code class="docutils literal notranslate"><span class="pre">init_client()</span></code> first.</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="pytest_texts_score.client.init_client">
<span class="sig-prename descclassname"><span class="pre">pytest_texts_score.client.</span></span><span class="sig-name descname"><span class="pre">init_client</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">config</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Config</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">AzureOpenAI</span></span></span><a class="reference internal" href="_modules/pytest_texts_score/client.html#init_client"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#pytest_texts_score.client.init_client" title="Link to this definition">¶</a></dt>
<dd><p>Initialize and store the global AzureOpenAI client.</p>
<p>This function uses the provided pytest configuration object to instantiate
the <code class="docutils literal notranslate"><span class="pre">AzureOpenAI</span></code> client. The created client instance is stored in a
global variable for later retrieval via <code class="docutils literal notranslate"><span class="pre">get_client()</span></code>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>config</strong> (<em>pytest.Config</em>) – The pytest config object containing LLM settings.</p>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>The newly created <code class="docutils literal notranslate"><span class="pre">AzureOpenAI</span></code> client instance.</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>AzureOpenAI</p>
</dd>
</dl>
</dd></dl>

</section>
<section id="module-pytest_texts_score.communication">
<span id="pytest-texts-score-communication-module"></span><h2>pytest_texts_score.communication module<a class="headerlink" href="#module-pytest_texts_score.communication" title="Link to this heading">¶</a></h2>
<dl class="py function">
<dt class="sig sig-object py" id="pytest_texts_score.communication.evaluate_questions">
<span class="sig-prename descclassname"><span class="pre">pytest_texts_score.communication.</span></span><span class="sig-name descname"><span class="pre">evaluate_questions</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">answer_text</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">questions_text</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">list</span><span class="p"><span class="pre">[</span></span><span class="pre">dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Any</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span></span><a class="reference internal" href="_modules/pytest_texts_score/communication.html#evaluate_questions"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#pytest_texts_score.communication.evaluate_questions" title="Link to this definition">¶</a></dt>
<dd><p>Evaluate how well a text answers a list of questions using the LLM.</p>
<p>This function sends the <code class="docutils literal notranslate"><span class="pre">answer_text</span></code> and a JSON string of
<code class="docutils literal notranslate"><span class="pre">questions_text</span></code> to the configured Azure OpenAI model. The model is
prompted to answer each question based on the text and provide a numeric
score. The function parses the JSON response and returns the list of
answers. It also handles and warns about responses that might include
markdown <a href="#id1"><span class="problematic" id="id2">``</span></a><a href="#id3"><span class="problematic" id="id4">`</span></a>json tags.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>answer_text</strong> (<em>str</em>) – The text to use for answering the questions.</p></li>
<li><p><strong>questions_text</strong> (<em>str</em>) – A JSON string representing the list of questions.</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>A list of dictionaries, where each dictionary contains a
‘question’ and its corresponding ‘answer’ score.</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>list[dict[str, Any]]</p>
</dd>
<dt class="field-even">Raises<span class="colon">:</span></dt>
<dd class="field-even"><ul class="simple">
<li><p><strong>ValueError</strong> – If the LLM response is not valid JSON or cannot be parsed.</p></li>
<li><p><strong>openai.APIError</strong> – If the API call to the LLM fails.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="pytest_texts_score.communication.make_questions">
<span class="sig-prename descclassname"><span class="pre">pytest_texts_score.communication.</span></span><span class="sig-name descname"><span class="pre">make_questions</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">base_text</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">str</span></span></span><a class="reference internal" href="_modules/pytest_texts_score/communication.html#make_questions"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#pytest_texts_score.communication.make_questions" title="Link to this definition">¶</a></dt>
<dd><p>Generate questions from a given text using the LLM.</p>
<p>This function sends the <code class="docutils literal notranslate"><span class="pre">base_text</span></code> to the configured Azure OpenAI model
with a system prompt designed to elicit factual yes/no questions. It
retrieves the global configuration and client instance to make the API call.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>base_text</strong> (<em>str</em>) – The text from which to generate questions.</p>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>A JSON string containing the generated questions. Returns an empty
string if the model response content is empty.</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>str</p>
</dd>
<dt class="field-even">Raises<span class="colon">:</span></dt>
<dd class="field-even"><p><strong>openai.APIError</strong> – If the API call to the LLM fails.</p>
</dd>
</dl>
</dd></dl>

</section>
<section id="module-pytest_texts_score.evaluate_score">
<span id="pytest-texts-score-evaluate-score-module"></span><h2>pytest_texts_score.evaluate_score module<a class="headerlink" href="#module-pytest_texts_score.evaluate_score" title="Link to this heading">¶</a></h2>
<dl class="py class">
<dt class="sig sig-object py" id="pytest_texts_score.evaluate_score.AggType">
<span class="property"><span class="k"><span class="pre">class</span></span><span class="w"> </span></span><span class="sig-prename descclassname"><span class="pre">pytest_texts_score.evaluate_score.</span></span><span class="sig-name descname"><span class="pre">AggType</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">values</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pytest_texts_score/evaluate_score.html#AggType"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#pytest_texts_score.evaluate_score.AggType" title="Link to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">Enum</span></code></p>
<p>Aggregation types for recall scores.</p>
<dl class="py attribute">
<dt class="sig sig-object py" id="pytest_texts_score.evaluate_score.AggType.AVERAGE">
<span class="sig-name descname"><span class="pre">AVERAGE</span></span><span class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">'average'</span></span><a class="headerlink" href="#pytest_texts_score.evaluate_score.AggType.AVERAGE" title="Link to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="pytest_texts_score.evaluate_score.AggType.MAXIMUM">
<span class="sig-name descname"><span class="pre">MAXIMUM</span></span><span class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">'maximum'</span></span><a class="headerlink" href="#pytest_texts_score.evaluate_score.AggType.MAXIMUM" title="Link to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="pytest_texts_score.evaluate_score.AggType.MEAN">
<span class="sig-name descname"><span class="pre">MEAN</span></span><span class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">'mean'</span></span><a class="headerlink" href="#pytest_texts_score.evaluate_score.AggType.MEAN" title="Link to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="pytest_texts_score.evaluate_score.AggType.MEDIAN">
<span class="sig-name descname"><span class="pre">MEDIAN</span></span><span class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">'median'</span></span><a class="headerlink" href="#pytest_texts_score.evaluate_score.AggType.MEDIAN" title="Link to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="pytest_texts_score.evaluate_score.AggType.MINIMUM">
<span class="sig-name descname"><span class="pre">MINIMUM</span></span><span class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">'minimum'</span></span><a class="headerlink" href="#pytest_texts_score.evaluate_score.AggType.MINIMUM" title="Link to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="py data">
<dt class="sig sig-object py" id="pytest_texts_score.evaluate_score.MAXIMAL_RETRY_ON_ERROR">
<span class="sig-prename descclassname"><span class="pre">pytest_texts_score.evaluate_score.</span></span><span class="sig-name descname"><span class="pre">MAXIMAL_RETRY_ON_ERROR</span></span><span class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">5</span></span><a class="headerlink" href="#pytest_texts_score.evaluate_score.MAXIMAL_RETRY_ON_ERROR" title="Link to this definition">¶</a></dt>
<dd><p>The maximum number of times to retry an LLM call upon failure before raising an exception.</p>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="pytest_texts_score.evaluate_score.ScoreType">
<span class="property"><span class="k"><span class="pre">class</span></span><span class="w"> </span></span><span class="sig-prename descclassname"><span class="pre">pytest_texts_score.evaluate_score.</span></span><span class="sig-name descname"><span class="pre">ScoreType</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">values</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pytest_texts_score/evaluate_score.html#ScoreType"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#pytest_texts_score.evaluate_score.ScoreType" title="Link to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">Enum</span></code></p>
<dl class="py attribute">
<dt class="sig sig-object py" id="pytest_texts_score.evaluate_score.ScoreType.F1">
<span class="sig-name descname"><span class="pre">F1</span></span><span class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">'f1'</span></span><a class="headerlink" href="#pytest_texts_score.evaluate_score.ScoreType.F1" title="Link to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="pytest_texts_score.evaluate_score.ScoreType.PRECISION">
<span class="sig-name descname"><span class="pre">PRECISION</span></span><span class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">'precision'</span></span><a class="headerlink" href="#pytest_texts_score.evaluate_score.ScoreType.PRECISION" title="Link to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="pytest_texts_score.evaluate_score.ScoreType.RECALL">
<span class="sig-name descname"><span class="pre">RECALL</span></span><span class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">'recall'</span></span><a class="headerlink" href="#pytest_texts_score.evaluate_score.ScoreType.RECALL" title="Link to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="pytest_texts_score.evaluate_score.f1_score">
<span class="sig-prename descclassname"><span class="pre">pytest_texts_score.evaluate_score.</span></span><span class="sig-name descname"><span class="pre">f1_score</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">precision</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">recall</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">float</span></span></span><a class="reference internal" href="_modules/pytest_texts_score/evaluate_score.html#f1_score"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#pytest_texts_score.evaluate_score.f1_score" title="Link to this definition">¶</a></dt>
<dd><p>Calculate the F1 score from precision and recall.</p>
<p>Computes the harmonic mean of precision and recall. Returns 0 if both
precision and recall are 0 to avoid division by zero.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>precision</strong> (<em>float</em>) – The precision score (between 0.0 and 1.0).</p></li>
<li><p><strong>recall</strong> (<em>float</em>) – The recall score (between 0.0 and 1.0).</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>The F1 score.</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>float</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="pytest_texts_score.evaluate_score.score_one_side">
<span class="sig-prename descclassname"><span class="pre">pytest_texts_score.evaluate_score.</span></span><span class="sig-name descname"><span class="pre">score_one_side</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">base_text</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">answer_text</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">retry_on_error</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">float</span></span></span><a class="reference internal" href="_modules/pytest_texts_score/evaluate_score.html#score_one_side"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#pytest_texts_score.evaluate_score.score_one_side" title="Link to this definition">¶</a></dt>
<dd><p>Calculate a one-sided score by generating questions from one text and answering with another.</p>
<p>This is a fundamental building block for both precision and recall calculations.
It generates a set of questions based on <code class="docutils literal notranslate"><span class="pre">base_text</span></code> and then evaluates
how well <code class="docutils literal notranslate"><span class="pre">answer_text</span></code> can answer them. The final score is the average of
the answer scores. This process forms
the basis for calculating both precision and recall.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>base_text</strong> (<em>str</em>) – The text to generate questions from.</p></li>
<li><p><strong>answer_text</strong> (<em>str</em>) – The text to answer the questions with.</p></li>
<li><p><strong>retry_on_error</strong> (<em>bool</em>) – Whether to retry LLM calls on failure. Defaults to <code class="docutils literal notranslate"><span class="pre">True</span></code>.</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>The average score from the evaluation.</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>float</p>
</dd>
<dt class="field-even">Raises<span class="colon">:</span></dt>
<dd class="field-even"><p><strong>Exception</strong> – If the operation fails after the maximum number of retries.</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="pytest_texts_score.evaluate_score.scores_agg">
<span class="sig-prename descclassname"><span class="pre">pytest_texts_score.evaluate_score.</span></span><span class="sig-name descname"><span class="pre">scores_agg</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">scores</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">list</span><span class="p"><span class="pre">[</span></span><span class="pre">float</span><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">agg_type</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference internal" href="#pytest_texts_score.evaluate_score.AggType" title="pytest_texts_score.evaluate_score.AggType"><span class="pre">AggType</span></a><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">Literal</span><span class="p"><span class="pre">[</span></span><span class="s"><span class="pre">'minimum'</span></span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="s"><span class="pre">'maximum'</span></span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="s"><span class="pre">'median'</span></span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="s"><span class="pre">'average'</span></span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="s"><span class="pre">'mean'</span></span><span class="p"><span class="pre">]</span></span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">float</span></span></span><a class="reference internal" href="_modules/pytest_texts_score/evaluate_score.html#scores_agg"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#pytest_texts_score.evaluate_score.scores_agg" title="Link to this definition">¶</a></dt>
<dd><p>Aggregate a list of scores using a specified method.</p>
<p>This function takes a list of numeric scores and applies an aggregation
function (min, max, median, or mean/average) to produce a single
summary score.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>scores</strong> (<em>list</em><em>[</em><em>float</em><em>]</em>) – A list of scores to aggregate.</p></li>
<li><p><strong>agg_type</strong> (<a class="reference internal" href="#pytest_texts_score.evaluate_score.AggType" title="pytest_texts_score.evaluate_score.AggType"><em>AggType</em></a><em> | </em><em>Literal</em><em>[</em><em>&quot;minimum&quot;</em><em>, </em><em>&quot;maximum&quot;</em><em>, </em><em>&quot;median&quot;</em><em>, </em><em>&quot;average&quot;</em><em>, </em><em>&quot;mean&quot;</em><em>]</em>) – The aggregation method to use.</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>The aggregated score.</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>float</p>
</dd>
<dt class="field-even">Raises<span class="colon">:</span></dt>
<dd class="field-even"><p><strong>ValueError</strong> – If an unknown aggregation type is provided.</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="pytest_texts_score.evaluate_score.texts_agg_f1">
<span class="sig-prename descclassname"><span class="pre">pytest_texts_score.evaluate_score.</span></span><span class="sig-name descname"><span class="pre">texts_agg_f1</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">expected</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">given</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">generate_questions</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">generate_answers_per_questions</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">agg_type</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference internal" href="#pytest_texts_score.evaluate_score.AggType" title="pytest_texts_score.evaluate_score.AggType"><span class="pre">AggType</span></a><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">Literal</span><span class="p"><span class="pre">[</span></span><span class="s"><span class="pre">'minimum'</span></span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="s"><span class="pre">'maximum'</span></span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="s"><span class="pre">'median'</span></span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="s"><span class="pre">'average'</span></span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="s"><span class="pre">'mean'</span></span><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">retry_on_error</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">float</span></span></span><a class="reference internal" href="_modules/pytest_texts_score/evaluate_score.html#texts_agg_f1"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#pytest_texts_score.evaluate_score.texts_agg_f1" title="Link to this definition">¶</a></dt>
<dd><p>Calculate an aggregated F1 score over multiple runs.</p>
<p>This function first generates multiple F1 scores by calling
<code class="docutils literal notranslate"><span class="pre">texts_multiple_f1</span></code> and then aggregates these scores using the
specified <code class="docutils literal notranslate"><span class="pre">agg_type</span></code>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>expected</strong> (<em>str</em>) – The reference text.</p></li>
<li><p><strong>given</strong> (<em>str</em>) – The text to evaluate.</p></li>
<li><p><strong>generate_questions</strong> (<em>int</em>) – The number of times to generate a new set of questions.</p></li>
<li><p><strong>generate_answers_per_questions</strong> (<em>int</em>) – The number of times to evaluate answers for each set of questions.</p></li>
<li><p><strong>agg_type</strong> (<a class="reference internal" href="#pytest_texts_score.evaluate_score.AggType" title="pytest_texts_score.evaluate_score.AggType"><em>AggType</em></a><em> | </em><em>Literal</em><em>[</em><em>&quot;minimum&quot;</em><em>, </em><em>&quot;maximum&quot;</em><em>, </em><em>&quot;median&quot;</em><em>, </em><em>&quot;average&quot;</em><em>, </em><em>&quot;mean&quot;</em><em>]</em>) – The aggregation method to use on the collected scores.</p></li>
<li><p><strong>retry_on_error</strong> (<em>bool</em>) – Whether to retry LLM calls on failure. Defaults to <code class="docutils literal notranslate"><span class="pre">True</span></code>.</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>The final aggregated F1 score.</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>float</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="pytest_texts_score.evaluate_score.texts_agg_precision">
<span class="sig-prename descclassname"><span class="pre">pytest_texts_score.evaluate_score.</span></span><span class="sig-name descname"><span class="pre">texts_agg_precision</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">expected</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">given</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">generate_questions</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">generate_answers_per_questions</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">agg_type</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference internal" href="#pytest_texts_score.evaluate_score.AggType" title="pytest_texts_score.evaluate_score.AggType"><span class="pre">AggType</span></a><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">Literal</span><span class="p"><span class="pre">[</span></span><span class="s"><span class="pre">'minimum'</span></span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="s"><span class="pre">'maximum'</span></span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="s"><span class="pre">'median'</span></span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="s"><span class="pre">'average'</span></span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="s"><span class="pre">'mean'</span></span><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">retry_on_error</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">float</span></span></span><a class="reference internal" href="_modules/pytest_texts_score/evaluate_score.html#texts_agg_precision"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#pytest_texts_score.evaluate_score.texts_agg_precision" title="Link to this definition">¶</a></dt>
<dd><p>Calculate an aggregated precision score over multiple runs.</p>
<p>This function first generates multiple precision scores by calling
<code class="docutils literal notranslate"><span class="pre">texts_multiple_precision</span></code> and then aggregates these scores using the
specified <code class="docutils literal notranslate"><span class="pre">agg_type</span></code>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>expected</strong> (<em>str</em>) – The reference text.</p></li>
<li><p><strong>given</strong> (<em>str</em>) – The text to evaluate.</p></li>
<li><p><strong>generate_questions</strong> (<em>int</em>) – The number of times to generate a new set of questions.</p></li>
<li><p><strong>generate_answers_per_questions</strong> (<em>int</em>) – The number of times to evaluate answers for each set of questions.</p></li>
<li><p><strong>agg_type</strong> (<a class="reference internal" href="#pytest_texts_score.evaluate_score.AggType" title="pytest_texts_score.evaluate_score.AggType"><em>AggType</em></a><em> | </em><em>Literal</em><em>[</em><em>&quot;minimum&quot;</em><em>, </em><em>&quot;maximum&quot;</em><em>, </em><em>&quot;median&quot;</em><em>, </em><em>&quot;average&quot;</em><em>, </em><em>&quot;mean&quot;</em><em>]</em>) – The aggregation method to use on the collected scores.</p></li>
<li><p><strong>retry_on_error</strong> (<em>bool</em>) – Whether to retry LLM calls on failure. Defaults to <code class="docutils literal notranslate"><span class="pre">True</span></code>.</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>The final aggregated precision score.</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>float</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="pytest_texts_score.evaluate_score.texts_agg_recall">
<span class="sig-prename descclassname"><span class="pre">pytest_texts_score.evaluate_score.</span></span><span class="sig-name descname"><span class="pre">texts_agg_recall</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">expected</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">given</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">generate_questions</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">generate_answers_per_questions</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">agg_type</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference internal" href="#pytest_texts_score.evaluate_score.AggType" title="pytest_texts_score.evaluate_score.AggType"><span class="pre">AggType</span></a><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">Literal</span><span class="p"><span class="pre">[</span></span><span class="s"><span class="pre">'minimum'</span></span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="s"><span class="pre">'maximum'</span></span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="s"><span class="pre">'median'</span></span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="s"><span class="pre">'average'</span></span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="s"><span class="pre">'mean'</span></span><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">retry_on_error</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">float</span></span></span><a class="reference internal" href="_modules/pytest_texts_score/evaluate_score.html#texts_agg_recall"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#pytest_texts_score.evaluate_score.texts_agg_recall" title="Link to this definition">¶</a></dt>
<dd><p>Calculate an aggregated recall score over multiple runs.</p>
<p>This function first generates multiple recall scores by calling
<code class="docutils literal notranslate"><span class="pre">texts_multiple_recall</span></code> and then aggregates these scores using the
specified <code class="docutils literal notranslate"><span class="pre">agg_type</span></code>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>expected</strong> (<em>str</em>) – The reference text.</p></li>
<li><p><strong>given</strong> (<em>str</em>) – The text to evaluate.</p></li>
<li><p><strong>generate_questions</strong> (<em>int</em>) – The number of times to generate a new set of questions.</p></li>
<li><p><strong>generate_answers_per_questions</strong> (<em>int</em>) – The number of times to evaluate answers for each set of questions.</p></li>
<li><p><strong>agg_type</strong> (<a class="reference internal" href="#pytest_texts_score.evaluate_score.AggType" title="pytest_texts_score.evaluate_score.AggType"><em>AggType</em></a><em> | </em><em>Literal</em><em>[</em><em>&quot;minimum&quot;</em><em>, </em><em>&quot;maximum&quot;</em><em>, </em><em>&quot;median&quot;</em><em>, </em><em>&quot;average&quot;</em><em>, </em><em>&quot;mean&quot;</em><em>]</em>) – The aggregation method to use on the collected scores.</p></li>
<li><p><strong>retry_on_error</strong> (<em>bool</em>) – Whether to retry LLM calls on failure. Defaults to <code class="docutils literal notranslate"><span class="pre">True</span></code>.</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>The final aggregated recall score.</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>float</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="pytest_texts_score.evaluate_score.texts_evaluate_f1">
<span class="sig-prename descclassname"><span class="pre">pytest_texts_score.evaluate_score.</span></span><span class="sig-name descname"><span class="pre">texts_evaluate_f1</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">expected</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">given</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">retry_on_error</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">float</span></span></span><a class="reference internal" href="_modules/pytest_texts_score/evaluate_score.html#texts_evaluate_f1"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#pytest_texts_score.evaluate_score.texts_evaluate_f1" title="Link to this definition">¶</a></dt>
<dd><p>Calculate the F1 score between two texts.</p>
<p>This function computes the F1 score by first calculating the precision and
recall between the <code class="docutils literal notranslate"><span class="pre">expected</span></code> and <code class="docutils literal notranslate"><span class="pre">given</span></code> texts. It serves as a
single-run evaluation of the harmonic mean of precision and recall.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>expected</strong> (<em>str</em>) – The reference text.</p></li>
<li><p><strong>given</strong> (<em>str</em>) – The text to be evaluated against the reference.</p></li>
<li><p><strong>retry_on_error</strong> (<em>bool</em>) – Whether to retry the LLM call on failure. Defaults to <code class="docutils literal notranslate"><span class="pre">True</span></code>.</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>The calculated F1 score.</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>float</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="pytest_texts_score.evaluate_score.texts_evaluate_precision">
<span class="sig-prename descclassname"><span class="pre">pytest_texts_score.evaluate_score.</span></span><span class="sig-name descname"><span class="pre">texts_evaluate_precision</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">expected</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">given</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">retry_on_error</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">float</span></span></span><a class="reference internal" href="_modules/pytest_texts_score/evaluate_score.html#texts_evaluate_precision"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#pytest_texts_score.evaluate_score.texts_evaluate_precision" title="Link to this definition">¶</a></dt>
<dd><p>Evaluate the precision score of the given text against the expected text.</p>
<p>Precision is calculated by generating questions from the <code class="docutils literal notranslate"><span class="pre">given</span></code> text and
checking how well they are answered by the <code class="docutils literal notranslate"><span class="pre">expected</span></code> text. This measures
how much of the information in the <code class="docutils literal notranslate"><span class="pre">given</span></code> text is also present in the
<code class="docutils literal notranslate"><span class="pre">expected</span></code> text.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>expected</strong> (<em>str</em>) – The reference text used for answering questions.</p></li>
<li><p><strong>given</strong> (<em>str</em>) – The text from which questions are generated.</p></li>
<li><p><strong>retry_on_error</strong> (<em>bool</em>) – Whether to retry the LLM call on failure. Defaults to <code class="docutils literal notranslate"><span class="pre">True</span></code>.</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>The calculated precision score.</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>float</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="pytest_texts_score.evaluate_score.texts_evaluate_recall">
<span class="sig-prename descclassname"><span class="pre">pytest_texts_score.evaluate_score.</span></span><span class="sig-name descname"><span class="pre">texts_evaluate_recall</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">expected</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">given</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">retry_on_error</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">float</span></span></span><a class="reference internal" href="_modules/pytest_texts_score/evaluate_score.html#texts_evaluate_recall"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#pytest_texts_score.evaluate_score.texts_evaluate_recall" title="Link to this definition">¶</a></dt>
<dd><p>Evaluate the recall score of the given text against the expected text.</p>
<p>Recall is calculated by generating questions from the <code class="docutils literal notranslate"><span class="pre">expected</span></code> text and
checking how well they are answered by the <code class="docutils literal notranslate"><span class="pre">given</span></code> text. This measures
how much of the information in the <code class="docutils literal notranslate"><span class="pre">expected</span></code> text is covered by the
<code class="docutils literal notranslate"><span class="pre">given</span></code> text.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>expected</strong> (<em>str</em>) – The reference text from which questions are generated.</p></li>
<li><p><strong>given</strong> (<em>str</em>) – The text used for answering questions.</p></li>
<li><p><strong>retry_on_error</strong> (<em>bool</em>) – Whether to retry the LLM call on failure. Defaults to <code class="docutils literal notranslate"><span class="pre">True</span></code>.</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>The calculated recall score.</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>float</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="pytest_texts_score.evaluate_score.texts_multiple_f1">
<span class="sig-prename descclassname"><span class="pre">pytest_texts_score.evaluate_score.</span></span><span class="sig-name descname"><span class="pre">texts_multiple_f1</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">expected</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">given</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">generate_questions</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">generate_answers_per_questions</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">score_only</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">retry_on_error</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">list</span><span class="p"><span class="pre">[</span></span><span class="pre">float</span><span class="p"><span class="pre">]</span></span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">list</span><span class="p"><span class="pre">[</span></span><span class="pre">tuple</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">float</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">float</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">float</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span></span><a class="reference internal" href="_modules/pytest_texts_score/evaluate_score.html#texts_multiple_f1"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#pytest_texts_score.evaluate_score.texts_multiple_f1" title="Link to this definition">¶</a></dt>
<dd><p>Perform multiple evaluation runs to get a list of F1 scores.</p>
<p>This function runs the F1 score evaluation multiple times to account for
variability in LLM responses. It generates new sets of questions for
precision and recall in each <code class="docutils literal notranslate"><span class="pre">generate_questions</span></code> loop, and for each set,
it evaluates answers <code class="docutils literal notranslate"><span class="pre">generate_answers_per_questions</span></code> times.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>expected</strong> (<em>str</em>) – The reference text.</p></li>
<li><p><strong>given</strong> (<em>str</em>) – The text to evaluate.</p></li>
<li><p><strong>generate_questions</strong> (<em>int</em>) – The number of times to generate a new set of questions.</p></li>
<li><p><strong>generate_answers_per_questions</strong> (<em>int</em>) – The number of times to evaluate answers for each set of questions.</p></li>
<li><p><strong>score_only</strong> (<em>bool</em>) – If <code class="docutils literal notranslate"><span class="pre">True</span></code>, returns only a list of F1 scores. If <code class="docutils literal notranslate"><span class="pre">False</span></code>, returns a list of tuples with detailed run info. Defaults to <code class="docutils literal notranslate"><span class="pre">True</span></code>.</p></li>
<li><p><strong>retry_on_error</strong> (<em>bool</em>) – Whether to retry LLM calls on failure. Defaults to <code class="docutils literal notranslate"><span class="pre">True</span></code>.</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>A list of F1 scores, or a list of tuples <code class="docutils literal notranslate"><span class="pre">(question_run,</span> <span class="pre">answer_run,</span> <span class="pre">precision,</span> <span class="pre">recall,</span> <span class="pre">f1_score)</span></code>.</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>list[float] | list[tuple[int, int, float, float, float]]</p>
</dd>
<dt class="field-even">Raises<span class="colon">:</span></dt>
<dd class="field-even"><p><strong>Exception</strong> – If the operation fails after the maximum number of retries.</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="pytest_texts_score.evaluate_score.texts_multiple_precision">
<span class="sig-prename descclassname"><span class="pre">pytest_texts_score.evaluate_score.</span></span><span class="sig-name descname"><span class="pre">texts_multiple_precision</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">expected</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">given</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">generate_questions</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">generate_answers_per_questions</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">score_only</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">retry_on_error</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">list</span><span class="p"><span class="pre">[</span></span><span class="pre">float</span><span class="p"><span class="pre">]</span></span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">list</span><span class="p"><span class="pre">[</span></span><span class="pre">tuple</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">float</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span></span><a class="reference internal" href="_modules/pytest_texts_score/evaluate_score.html#texts_multiple_precision"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#pytest_texts_score.evaluate_score.texts_multiple_precision" title="Link to this definition">¶</a></dt>
<dd><p>Perform multiple evaluation runs to get a list of precision scores.</p>
<p>This function runs the precision score evaluation multiple times. It
generates new sets of questions from the <code class="docutils literal notranslate"><span class="pre">given</span></code> text in each
<code class="docutils literal notranslate"><span class="pre">generate_questions</span></code> loop, and for each set, it evaluates answers
<code class="docutils literal notranslate"><span class="pre">generate_answers_per_questions</span></code> times using the <code class="docutils literal notranslate"><span class="pre">expected</span></code> text.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>expected</strong> (<em>str</em>) – The reference text for answering.</p></li>
<li><p><strong>given</strong> (<em>str</em>) – The text to generate questions from.</p></li>
<li><p><strong>generate_questions</strong> (<em>int</em>) – The number of times to generate a new set of questions.</p></li>
<li><p><strong>generate_answers_per_questions</strong> (<em>int</em>) – The number of times to evaluate answers for each set of questions.</p></li>
<li><p><strong>score_only</strong> (<em>bool</em>) – If <code class="docutils literal notranslate"><span class="pre">True</span></code>, returns only a list of precision scores. If <code class="docutils literal notranslate"><span class="pre">False</span></code>, returns a list of tuples with detailed run info. Defaults to <code class="docutils literal notranslate"><span class="pre">True</span></code>.</p></li>
<li><p><strong>retry_on_error</strong> (<em>bool</em>) – Whether to retry LLM calls on failure. Defaults to <code class="docutils literal notranslate"><span class="pre">True</span></code>.</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>A list of precision scores, or a list of tuples <code class="docutils literal notranslate"><span class="pre">(question_run,</span> <span class="pre">answer_run,</span> <span class="pre">precision)</span></code>.</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>list[float] | list[tuple[int, int, float]]</p>
</dd>
<dt class="field-even">Raises<span class="colon">:</span></dt>
<dd class="field-even"><p><strong>Exception</strong> – If the operation fails after the maximum number of retries.</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="pytest_texts_score.evaluate_score.texts_multiple_recall">
<span class="sig-prename descclassname"><span class="pre">pytest_texts_score.evaluate_score.</span></span><span class="sig-name descname"><span class="pre">texts_multiple_recall</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">expected</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">given</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">generate_questions</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">generate_answers_per_questions</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">score_only</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">retry_on_error</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">list</span><span class="p"><span class="pre">[</span></span><span class="pre">float</span><span class="p"><span class="pre">]</span></span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">list</span><span class="p"><span class="pre">[</span></span><span class="pre">tuple</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">float</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span></span><a class="reference internal" href="_modules/pytest_texts_score/evaluate_score.html#texts_multiple_recall"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#pytest_texts_score.evaluate_score.texts_multiple_recall" title="Link to this definition">¶</a></dt>
<dd><p>Perform multiple evaluation runs to get a list of recall scores.</p>
<p>This function runs the recall score evaluation multiple times. It generates
new sets of questions from the <code class="docutils literal notranslate"><span class="pre">expected</span></code> text in each
<code class="docutils literal notranslate"><span class="pre">generate_questions</span></code> loop, and for each set, it evaluates answers
<code class="docutils literal notranslate"><span class="pre">generate_answers_per_questions</span></code> times using the <code class="docutils literal notranslate"><span class="pre">given</span></code> text.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>expected</strong> (<em>str</em>) – The reference text to generate questions from.</p></li>
<li><p><strong>given</strong> (<em>str</em>) – The text for answering.</p></li>
<li><p><strong>generate_questions</strong> (<em>int</em>) – The number of times to generate a new set of questions.</p></li>
<li><p><strong>generate_answers_per_questions</strong> (<em>int</em>) – The number of times to evaluate answers for each set of questions.</p></li>
<li><p><strong>score_only</strong> (<em>bool</em>) – If <code class="docutils literal notranslate"><span class="pre">True</span></code>, returns only a list of recall scores. If <code class="docutils literal notranslate"><span class="pre">False</span></code>, returns a list of tuples with detailed run info. Defaults to <code class="docutils literal notranslate"><span class="pre">True</span></code>.</p></li>
<li><p><strong>retry_on_error</strong> (<em>bool</em>) – Whether to retry LLM calls on failure. Defaults to <code class="docutils literal notranslate"><span class="pre">True</span></code>.</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>A list of recall scores, or a list of tuples <code class="docutils literal notranslate"><span class="pre">(question_run,</span> <span class="pre">answer_run,</span> <span class="pre">recall)</span></code>.</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>list[float] | list[tuple[int, int, float]]</p>
</dd>
<dt class="field-even">Raises<span class="colon">:</span></dt>
<dd class="field-even"><p><strong>Exception</strong> – If the operation fails after the maximum number of retries.</p>
</dd>
</dl>
</dd></dl>

</section>
<section id="module-pytest_texts_score.plugin">
<span id="pytest-texts-score-plugin-module"></span><h2>pytest_texts_score.plugin module<a class="headerlink" href="#module-pytest_texts_score.plugin" title="Link to this heading">¶</a></h2>
<dl class="py function">
<dt class="sig sig-object py" id="pytest_texts_score.plugin.get_config">
<span class="sig-prename descclassname"><span class="pre">pytest_texts_score.plugin.</span></span><span class="sig-name descname"><span class="pre">get_config</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Config</span></span></span><a class="reference internal" href="_modules/pytest_texts_score/plugin.html#get_config"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#pytest_texts_score.plugin.get_config" title="Link to this definition">¶</a></dt>
<dd><p>Return the initialized pytest configuration object.</p>
<p>Retrieves the globally stored <code class="docutils literal notranslate"><span class="pre">pytest.Config</span></code> object, which contains
the resolved LLM configuration. This function should be called after
<code class="docutils literal notranslate"><span class="pre">pytest_configure</span></code> has run.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns<span class="colon">:</span></dt>
<dd class="field-odd"><p>The pytest config object.</p>
</dd>
<dt class="field-even">Return type<span class="colon">:</span></dt>
<dd class="field-even"><p>pytest.Config</p>
</dd>
<dt class="field-odd">Raises<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>RuntimeError</strong> – If the configuration has not been initialized.</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="pytest_texts_score.plugin.mask_api_key">
<span class="sig-prename descclassname"><span class="pre">pytest_texts_score.plugin.</span></span><span class="sig-name descname"><span class="pre">mask_api_key</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">key</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">str</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span></span><a class="reference internal" href="_modules/pytest_texts_score/plugin.html#mask_api_key"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#pytest_texts_score.plugin.mask_api_key" title="Link to this definition">¶</a></dt>
<dd><p>Mask an API key for safe display.</p>
<p>Replaces all but the first character of the API key with asterisks (<code class="docutils literal notranslate"><span class="pre">*</span></code>)
to prevent leaking sensitive information in logs or reports.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>key</strong> (<em>Optional</em><em>[</em><em>str</em><em>]</em>) – The API key to mask.</p>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>The masked API key, or <code class="docutils literal notranslate"><span class="pre">None</span></code> if the input was <code class="docutils literal notranslate"><span class="pre">None</span></code>.</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>Optional[str]</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="pytest_texts_score.plugin.pytest_addoption">
<span class="sig-prename descclassname"><span class="pre">pytest_texts_score.plugin.</span></span><span class="sig-name descname"><span class="pre">pytest_addoption</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">parser</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Parser</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">None</span></span></span><a class="reference internal" href="_modules/pytest_texts_score/plugin.html#pytest_addoption"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#pytest_texts_score.plugin.pytest_addoption" title="Link to this definition">¶</a></dt>
<dd><p>Add command-line and .ini options for LLM configuration to pytest.</p>
<p>This hook implementation defines various options to configure the Azure
OpenAI client, such as API key, endpoint, model, and other parameters.
Options can be provided via the command line or a <code class="docutils literal notranslate"><span class="pre">pytest.ini</span></code> file.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>parser</strong> (<em>pytest.Parser</em>) – The pytest option parser.</p>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>None.</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="pytest_texts_score.plugin.pytest_configure">
<span class="sig-prename descclassname"><span class="pre">pytest_texts_score.plugin.</span></span><span class="sig-name descname"><span class="pre">pytest_configure</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">config</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Config</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">None</span></span></span><a class="reference internal" href="_modules/pytest_texts_score/plugin.html#pytest_configure"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#pytest_texts_score.plugin.pytest_configure" title="Link to this definition">¶</a></dt>
<dd><p>Resolve LLM config (CLI &gt; ini &gt; default) and initialize the client.</p>
<p>This hook is called after command line and configuration files are parsed.
It resolves the final configuration values by prioritizing command-line
options over <code class="docutils literal notranslate"><span class="pre">.ini</span></code> file settings, and then over default values. It
validates that all required settings are present and then initializes the
global LLM client.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>config</strong> (<em>pytest.Config</em>) – The pytest config object.</p>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>None.</p>
</dd>
<dt class="field-odd">Raises<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>pytest.UsageError</strong> – If any required configuration values are missing.</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="pytest_texts_score.plugin.pytest_report_header">
<span class="sig-prename descclassname"><span class="pre">pytest_texts_score.plugin.</span></span><span class="sig-name descname"><span class="pre">pytest_report_header</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">config</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Config</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">str</span></span></span><a class="reference internal" href="_modules/pytest_texts_score/plugin.html#pytest_report_header"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#pytest_texts_score.plugin.pytest_report_header" title="Link to this definition">¶</a></dt>
<dd><p>Add LLM configuration details to the pytest report header.</p>
<p>This hook provides a custom string to be displayed in the header of the
test report, showing the resolved LLM configuration parameters for the
current test run. The API key is masked for security.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>config</strong> (<em>pytest.Config</em>) – The pytest config object.</p>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>A string to be included in the report header.</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>str</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="pytest_texts_score.plugin.texts_score">
<span class="sig-prename descclassname"><span class="pre">pytest_texts_score.plugin.</span></span><span class="sig-name descname"><span class="pre">texts_score</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Callable</span><span class="p"><span class="pre">]</span></span></span></span><a class="reference internal" href="_modules/pytest_texts_score/plugin.html#texts_score"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#pytest_texts_score.plugin.texts_score" title="Link to this definition">¶</a></dt>
<dd><p>Provide access to text comparison helper functions as a fixture.</p>
<p>This fixture returns a dictionary of callable functions for text scoring
and evaluation. These functions include various aggregation and expectation
helpers for metrics like F1 score, precision, recall, completeness, and correctness.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns<span class="colon">:</span></dt>
<dd class="field-odd"><p>A dictionary mapping function names to callable helper functions.</p>
</dd>
<dt class="field-even">Return type<span class="colon">:</span></dt>
<dd class="field-even"><p>dict[str, Callable]</p>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>This fixture returns a dictionary of functions rather than exposing them
globally.</p>
</div>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="pytest_texts_score.plugin.texts_score_client">
<span class="sig-prename descclassname"><span class="pre">pytest_texts_score.plugin.</span></span><span class="sig-name descname"><span class="pre">texts_score_client</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">AzureOpenAI</span></span></span><a class="reference internal" href="_modules/pytest_texts_score/plugin.html#texts_score_client"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#pytest_texts_score.plugin.texts_score_client" title="Link to this definition">¶</a></dt>
<dd><p>Provide access to the initialized LLM client as a fixture.</p>
<p>This session-scoped fixture allows tests to get the configured
<code class="docutils literal notranslate"><span class="pre">AzureOpenAI</span></code> client instance.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns<span class="colon">:</span></dt>
<dd class="field-odd"><p>The initialized <code class="docutils literal notranslate"><span class="pre">AzureOpenAI</span></code> client.</p>
</dd>
<dt class="field-even">Return type<span class="colon">:</span></dt>
<dd class="field-even"><p>AzureOpenAI</p>
</dd>
</dl>
</dd></dl>

</section>
<section id="module-pytest_texts_score.prompts">
<span id="pytest-texts-score-prompts-module"></span><h2>pytest_texts_score.prompts module<a class="headerlink" href="#module-pytest_texts_score.prompts" title="Link to this heading">¶</a></h2>
<p>Prompt templates used by pytest-texts-score.
These prompts are carefully engineered to guide the LLM’s behavior for question generation and evaluation. Modifying them may have significant impacts on the scoring results.</p>
<dl class="py function">
<dt class="sig sig-object py" id="pytest_texts_score.prompts.get_system_answers_prompt">
<span class="sig-prename descclassname"><span class="pre">pytest_texts_score.prompts.</span></span><span class="sig-name descname"><span class="pre">get_system_answers_prompt</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">str</span></span></span><a class="reference internal" href="_modules/pytest_texts_score/prompts.html#get_system_answers_prompt"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#pytest_texts_score.prompts.get_system_answers_prompt" title="Link to this definition">¶</a></dt>
<dd><p>Get the system prompt for answering questions.</p>
<p>This function returns the predefined system prompt that instructs the LLM
on how to answer a list of questions based on a given text, using a numeric scoring system.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns<span class="colon">:</span></dt>
<dd class="field-odd"><p>The question answering prompt string.</p>
</dd>
<dt class="field-even">Return type<span class="colon">:</span></dt>
<dd class="field-even"><p>str</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="pytest_texts_score.prompts.get_system_questions_prompt">
<span class="sig-prename descclassname"><span class="pre">pytest_texts_score.prompts.</span></span><span class="sig-name descname"><span class="pre">get_system_questions_prompt</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">str</span></span></span><a class="reference internal" href="_modules/pytest_texts_score/prompts.html#get_system_questions_prompt"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#pytest_texts_score.prompts.get_system_questions_prompt" title="Link to this definition">¶</a></dt>
<dd><p>Get the system prompt for generating questions.</p>
<p>This function returns the predefined system prompt that instructs the LLM
on how to generate factual yes/no questions from a given text.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns<span class="colon">:</span></dt>
<dd class="field-odd"><p>The question generation prompt string.</p>
</dd>
<dt class="field-even">Return type<span class="colon">:</span></dt>
<dd class="field-even"><p>str</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="pytest_texts_score.prompts.get_user_answers_prompt">
<span class="sig-prename descclassname"><span class="pre">pytest_texts_score.prompts.</span></span><span class="sig-name descname"><span class="pre">get_user_answers_prompt</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">answer_text</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">questions_text</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">str</span></span></span><a class="reference internal" href="_modules/pytest_texts_score/prompts.html#get_user_answers_prompt"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#pytest_texts_score.prompts.get_user_answers_prompt" title="Link to this definition">¶</a></dt>
<dd><p>Create a user prompt for answering questions.</p>
<p>This function formats the text and the questions into a single prompt
that will be paired with the system answer prompt.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>answer_text</strong> (<em>str</em>) – The text to use for answering the questions.</p></li>
<li><p><strong>questions_text</strong> (<em>str</em>) – The JSON string of questions to be answered.</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>The formatted user prompt string.</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>str</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="pytest_texts_score.prompts.get_user_questions_prompt">
<span class="sig-prename descclassname"><span class="pre">pytest_texts_score.prompts.</span></span><span class="sig-name descname"><span class="pre">get_user_questions_prompt</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">text</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">str</span></span></span><a class="reference internal" href="_modules/pytest_texts_score/prompts.html#get_user_questions_prompt"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#pytest_texts_score.prompts.get_user_questions_prompt" title="Link to this definition">¶</a></dt>
<dd><p>Create a user prompt for question generation.</p>
<p>This function formats the user-provided text into a simple prompt
that will be paired with the system question prompt.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>text</strong> (<em>str</em>) – The text to generate questions from.</p>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>The formatted user prompt string.</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>str</p>
</dd>
</dl>
</dd></dl>

</section>
</section>


          </div>
          
        </div>
      </div>
      <div class="sphinxsidebar" role="navigation" aria-label="Main">
        <div class="sphinxsidebarwrapper">
<h1 class="logo"><a href="index.html">pytest-texts-score</a></h1>









<search id="searchbox" style="display: none" role="search">
    <div class="searchformwrapper">
    <form class="search" action="search.html" method="get">
      <input type="text" name="q" aria-labelledby="searchlabel" autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" placeholder="Search"/>
      <input type="submit" value="Go" />
    </form>
    </div>
</search>
<script>document.getElementById('searchbox').style.display = "block"</script><h3>Navigation</h3>
<p class="caption" role="heading"><span class="caption-text">Contents:</span></p>
<ul class="current">
<li class="toctree-l1 current"><a class="reference internal" href="modules.html">pytest_texts_score</a><ul class="current">
<li class="toctree-l2 current"><a class="current reference internal" href="#">pytest_texts_score package</a></li>
</ul>
</li>
</ul>

<div class="relations">
<h3>Related Topics</h3>
<ul>
  <li><a href="index.html">Documentation overview</a><ul>
  <li><a href="modules.html">pytest_texts_score</a><ul>
      <li>Previous: <a href="modules.html" title="previous chapter">pytest_texts_score</a></li>
  </ul></li>
  </ul></li>
</ul>
</div>








        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="footer">
      &#169;2025, Patrik Vodila.
      
      |
      Powered by <a href="https://www.sphinx-doc.org/">Sphinx 9.0.4</a>
      &amp; <a href="https://alabaster.readthedocs.io">Alabaster 1.0.0</a>
      
      |
      <a href="_sources/pytest_texts_score.rst.txt"
          rel="nofollow">Page source</a>
    </div>

    

    
  </body>
</html>